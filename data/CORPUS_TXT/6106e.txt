<heading>1)</heading>A cluster is a group of loosely coupled computers that work together as a complete system to give the appearance to a user of being a single unit. This is true even though each machine within the cluster, commonly known as a node, is fully complete itself as they all have a CPU, memory, and I/O facilities. Data communications provide the link between the machines in the cluster to allow for data transfer. The purpose of is to increase available computing power by combining the power of individual systems. Clustering can prove less expensive than purchasing a mainframe that will do a similar job. Clusters are inherently scalable because more nodes can be added to the cluster if more processing power is needed. Clusters have the ability to increase the available computing power by combining the power of individual systems, and since each computer can process data independently, the increase in power is roughly proportional to the number of nodes in the cluster. This scalability is possible due to the software controlling the cluster and the communication links between the nodes. Processes are distributed to other CPUs in other nodes in order to use their resources if they are idle. This can happen regardless of the number of nodes in the cluster as long as the software driving the cluster can handle it. A great advantage of a cluster is that if one node fails the entire system will remain in operation. The software controlling the system will try to rebalance the workload between the remaining nodes so that the efficiency of the system remains high. This shows that the system is incredibly robust. One or more nodes have to act as an interface to the entire system so that users can logon and perform tasks. If one or more of these nodes fails, although the entire system will not fail, productivity will not be as high as less people can use the system until the node is repaired or another is established as an interface node. There are two models used for clustering; the shared nothing model and the shared-disk model. In the shared nothing model each node consists of CPU(s), memory and local disk(s) and is connection point-to-point with other nodes via a high speed messaging link. Ideally the workload is divided between the nodes so that requests made for each will be approximately equal so that it is efficient as little communication is needed. For the user there will be faster access times to the disks for data as the data is kept local; bottlenecks will be avoided and so frustration will not occur thus increasing morale. The company will benefit from the model because faster access means that the users will be more productive in their work, and the costs for communications will be lower as it will not be as important/crucial in the system's efficiency. If data is stored locally for each node then if one node fails then the data stored on the local disk can be accessed from other nodes via the communication links so that the node isn't made useless. This will maintain the full power of the cluster so productivity of the company is maintained at a high level. Alternatively, the shared-disk model has data shared between nodes because there are disks that are accessible to every node. This causes issues of reading data whilst it is being updated but on the other hand there is greatly increased availability of data. If data is being stored on multiple disks there is always a chance of the data being incorrect as it is hard to keep data consistent when it is stored in many locations. However, data stored on a shared disk is only written to once and this eliminates the risk of inconsistencies and therefore incorrect data being read back to the user. This provides a better service to the users and greater credibility to the company which can lead to increased profits. Access time for this model is greatly increased if the shared disks are in a mirrored array. A request for a read is made, and the data blocks are read from across the disks, and then the data is reassembled before being returned to the user. This will again increase productivity for the user and revenues for the company implementing the system. <heading>2)</heading>An operating system is a program that controls the execution of application programs and acts as an interface between the user of the computer and the hardware. The operating system has several general functions including managing the hardware resources of the computer by allocating the resource's power to processes created by the user. It provides a user friendly interface so that hardware resources can be manipulated easily without an in depth knowledge of the hardware architecture. It also handles input/output devices as their functionality is common to the majority of applications. Therefore it is more efficient to include the controlling instructions once in the operating system so they can be called upon as necessary, instead of having the same code inside every application. The operating system controls the allocation of memory to programs. It pages out parts of the program if it is too big to fit in memory and ensures that one program in memory doesn't overwrite another. A mainframe is an expensive, high powered computer capable of supporting many users simultaneously. Single user machines and mainframes essentially share the same underlying principles but a mainframe will have more I/O devices, more memory, vast amounts of permanent storage usually on tapes, and multiple CPUs as much more processing power is needed. As a result of this, the operating systems running on single user machines and mainframes do have some similar characteristics, but mainframes need additional ones to cater for the vast workload and multiple users. Both operating systems will share basic characteristics such as a graphical user interface which use mice and menus to navigate through options and support for input and output devices using drivers, but an OS such as z/OS for an IBM mainframe will have more advanced features. It will have the ability to support hundreds of users working simultaneously all accessing applications, and all demanding the system resources. Therefore it will have methods of managing the workload, for example by dynamically allocating tasks based on priorities to ensure efficient use of the large memory available, and by assigning tasks to one of several CPUs in the mainframe which can then be switched to another CPU if necessary. Single user OSs, although they support multiple CPUs and workload management, are primarily designed to cope with only single CPU machines and smaller workloads from only single users. The OS will have to handle more complex resource logging an auditing from the vast number of users accessing the system, storing node used to logon, GID, UID numbers, permissions and levels of access; issues that aren't as important for a single user machine OS to have. The OS needs to include distributed file services in order to support Linux, Windows and OS2 file systems which may be apparent within the mainframe. An OS such as Windows 2000 would only need to support one file system possibly NTFS or FAT32 as distributed file systems are not an issue. The fetch-execute cycle is the sequence of actions that a CPU performs to execute each machine code instruction in a program. The operating system is ultimately responsible for the whole fetch-execute cycle as it manages all the hardware resources including the processor. Operating systems deal with the concept of processes which are defined as programs in execution. A process is initiated by the user via the operating system by a system call and it creates an address space for itself which is a list of memory locations where it can read/write too. Also associated with the process are a set of registers including a program counter and a stack pointer. During the fetch-execute cycle, the processor uses the memory address held in the program counter register to locate the instruction to be executed which is currently held in memory, and it increments the counter by one to point to the next instruction to be fetched. The memory loads the fetched instruction into an instruction register, ready for execution from the CPU. After the instruction has been executed there is a check for interrupts which takes priority over other instructions waiting. This cycle then repeats itself until all instructions have been executed. An interrupt is a temporary break in the CPU's normal execution of program instructions caused by an external event, performed in such a way that the process can be resumed. Interrupts are commonly generated for a variety of reasons, often by input and output controller to signify the normal completion of an operation, or on account of hardware failure caused by low power. They can also be generated by programs when illegal instructions are trying to be executed, or if a program is attempting to a divide a value by zero. The CPU has a number of control lines called interrupt lines which deal with instances described above. In the circumstance of a printer finishing its jobs, a signal is sent along these control lines which affects the fetch-execute cycle directly after the current instruction is executed. This suspends the program and saves the values currently in the registers in an area of memory known as the process control block (PCB), and jumps to an interrupt handling program unless interrupts have been disabled. From here, the interrupt handling program completes its task, maybe giving a message to the user reading "Print jobs complete", and returns control to the interrupted program. The values saved in memory are returned to the registers so that the fetch execute cycle can continue exactly as before. 