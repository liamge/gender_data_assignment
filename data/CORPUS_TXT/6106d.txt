<heading>1) </heading><heading>a) </heading>A UNIX process can be defined as a program, together with all the resources that are associated with that process as it is executed or as a single instance of a program running on a UNIX system; but the latter is more widely accepted. A process can start creating additional processes once it starts executing via a process known as 'forking' or 'spawning'; these are exact copies of the original process and are called 'child processes'. There are special processes called 'daemons' which run as the system starts and they perform general housekeeping tasks such as scheduling actions to happen at set times e.g. cron, but the majority are created when a program is executed. Each process has a unique PID number of identification which allows them to be traced and killed if necessary. <heading>b)</heading>Processes that interact with other processes are called cooperating processes and these are very common in modern systems. Processes communicate via pipes which are one-way mechanisms that allow two related processes to send a byte stream across a channel from one to the other. This is generally to allow the output of one process to become the input value of another. This can be seen in UNIX commands such as 'cat 'filename' | more', where the output of the 'cat' command becomes the input of the more 'command'. Using message queues is another way of processes to communicate whereby messages are placed into a queue with an identification number, data (message), and a private or public stamp much like a notice board. Private messages can only be read by the process that created the message or a child of it, but public message can be read by all processes. Several processes can read and write to this queue at once and messages can be read by number unlike piped communication whereby a whole stream needs to be received before a message can be read. Pipe naming works much the same in that processes that are not related can communicate. A named pipe has an access point which is a file and several pipes can write and read to it thus allowing data to be shared. <heading>c) </heading>Memory management is the planned organisation of programs and data in to memory with the goal of space being easy to find. However, there maybe more programs running than can fit into the available memory space available on a system, but it is desirable to have as many programs residing in memory as it increases machine performance. To aid the problem of memory management the idea of virtual storage was introduced to the multitasking system along with two memory management techniques called swapping and paging which both use this concept. The main different between paging and swapping is that swapping is more concerned with the scheduling of processes to be processed whereas paging deals more with storing the processes to be run. Swapping was introduced first to the UNIX system which is the process of using virtual memory to temporarily store data when physical memory is full when too many tasks are perceived to be competing for limited resources. Swapping aids in the scheduling by swapping out processes to virtual storage based on predefined priorities such as the length of time its had resided in memory. Processes are swapped back from virtual storage if they are ready to be run and there is sufficient space in the physical memory. The amount of space available in memory is calculated using complex algorithms designed to decide whether a process can be swapped back in its entirety. In contrast, paging is concerned more with memory allocation than scheduling and came in later versions of UNIX. Paging can be described as the transfer of pages between physical and virtual storage. The technique uses page tables to map between virtual address and physical addresses in the memory. Each process in physical memory only has its user structure (machine registers, accounting stack, state etc) and page tables with it, data tables and pages of text are brought in dynamically as referenced. This is very different to swapping as the whole process is moved in and out of physical and virtual memory when it is ready to be executed and there is enough available space. Paging is part of the fetch-execute cycle. The CPU queries the address register to find where in physical memory the next process resides. If it is not in the physical memory then the process address is mapped through the page table to the virtual memory and then the page needed is read back into physical memory in exchange for another page which is moved out, and then it is executed. This is slower than if the process is already in the physical storage. <heading>d)</heading>Microsoft does not make system calls publicly available so instead Win32 API is used to make system calls. Win32 API is a library containing procedures that make these calls for you so that you are indirectly making the system calls. The calls available do not change within latter version of windows but more calls are added for every release. Certain calls create kernel objects such as processes or files, and these return what is known as a handles to the caller that are specific to that object, which allow operations to be used on the objects according to its permissions. These operations may include create or destroy files, open, close and read. There are many procedures available in the library that make calls to allow manipulation of the graphical user interface of windows and associated functionality. <heading>2) </heading><heading>a)</heading>When the nodes on a Token Ring Network are first powered on at start up, one is designated as the active monitor which sends out a data packet every seven seconds around the ring to see if any nodes are down in order to diagnose a problem which would prevent data flow. If a node doesn't receive a packet from its nearest active upstream neighbour it creates a packet to send onto the network with he address of the fallen node so the ring can reconfigure itself logically. IBM's Token Ring is actually networked with a star topology even though the name suggests otherwise. However, the ring is actually logical and a Multistation Access Unit hosts the ring internally. Access to the network media is handled by the possession of a token that is continuously passed around the ring from node to node until one takes the token and replaces it with a 'busy' token and adds a message to be transmitted. The data travels around the ring in a one direction until the node with the correct NIC accepts the data and modifies the token with a 'copied' token that is returned to the sender as acknowledgement. At this point the node that transmitted places the original token back on the network and passes it onto the next node on the ring (nearest active downstream neighbour). <heading>b)</heading>CSMA/CD stands for Carrier Sensitive Multiple Access with Collision Detection, and it is a protocol that is apparent on all Ethernet Networks. As Ethernet Networks don't try and prevent collisions they deal with them using this protocol. Collisions occur when two nodes attempt to transmit simultaneously resulting in the two messages being sent interfering with each other and becoming scrambled. The CMSA listens to the line to detect whether the line is free before a node transmits. After it has transmitted, it continues to listen to the line. If a node detects a collision on the line, it immediately stops transmission, waits a short time, and then returns to listening to the line so it can retransmit. The amount of time each node waits after a collision is random before retransmission occurs. As a result of collisions, Ethernet is not suited to high traffic networks or ones with long connections. <heading>c) </heading>The main differences between the Token Ring and Ethernet architectures are the way they are connected and the way in which they transmit data. The Token Ring Network is wired in a star topology much like the majority of Ethernet Networks but the difference is the logical ring which resides inside the Multistation Access Unit which hosts the ring. The logical ring means that the Token Ring Network is harder to configure when initially implemented than the Ethernet and the hardware is more expensive as Ethernet only requires hubs, NICs, and cabling. The Token Ring Network uses an active architecture because each node has to pass data on to its nearest active downstream neighbour in a ring in order for it to reach its destination. In contrast, Ethernet uses a passive architecture as it waits and listens on the transmitting line for any messages that are waiting for it. Each packet of data has a MAC address on it and the corresponding NIC picks up the packet. Both methods get the data from A to B but Ethernet is typically faster when network traffic is low as nodes can transmit when they like; no token is needed, but faster flavours of Token Ring are being developed. However, when the network traffic increases the Ethernet architecture's performance is reduced as collisions occur. If two nodes attempt to transmit at once there is a collision and both have to resend when the line is free; obviously on a high traffic network this is not efficient. In contrast, the Token Ring architecture copes well with high traffic as collisions do not occur as nodes can only transmit when they are in possession of the token thus making it speedier than Ethernet. Token Ring is a fair network and prevents one node from hogging the network media. It doesn't allow a node that has just transmitted to transmit again in succession, and so the speed of the network is consistent at all times. Ethernet does not have a scheme like this and a node can flood the network with data and not allow others to transmit. This can be frustrating for other users as the network will appear slow. You could argue that Ethernet is more flexible than Token Ring as it can be wired as a Bus or Star topology giving you options with cabling but as prices are dropping rapidly, this is no longer becoming an issue, and the ability for a node to be added onto a bus network makes the architecture very scalable. Token Ring is stuck with the star configuration but this is standard across many architectures today, but it is not very scalable because the more nodes on the network, the less often the token is available and so sub netting is necessary. <heading>d)</heading>Today 80% of LAN connections installed in universities and within organisations use Ethernet. It is preferable over Token Ring because it has further advances in the size of the bandwidth with Gigabit Ethernet being common these days. Token Ring has made advances into larger bandwidths but not on the same scale as Ethernet. Ethernet is very flexible and it integrates well between LAN and WAN services which is much needed for multinational organisations. It is an inherently scalable architecture in the sense that as many nodes can be added as necessary with only a NIC and a cable necessary and minimal configuration (unless sub netting is necessary), or just a wireless NIC for a wireless connection which is very useful for organisations which maybe in growth. Token Ring is also scalable but only up to a certain point as the circle would become too large to be able to efficiently pass the token around. It is also proving a very affordable solution as hardware is very cheap compared to the Token Ring architecture which needs a multistation access unit in addition to cabling and NICs. Most PCs and laptops today come with a NIC included and so Ethernet only carries the cost of the cabling. <heading>3) </heading><heading>a) </heading>As IP-based networks can be of different sizes, the architects of the IP addressing system set up different classes of IP addresses to accommodate different sizes of networks. IP addresses exist within three distinct classes commonly known as class A, B, and C. The class in which an IP address resides, is determined by the first byte of the IP address or the digits before the first dot. Class A is used for very large networks allowing for more than 1.5 million hosts, and the first byte of the addresses covers the range from 0-127 meaning there can only be 127 class A networks e.g. ARPAnet. Class B is used for large companies and institutions, and the first byte covers the range from 128-191. There are 16,384 class B network addresses available each allowing for more than 65,000 host addresses. Class C ranges from 192-233. There are over 2 million network addresses available but they only supply 254 node addresses and are therefore ideally suited for small company or home networks. Two other classes of IP address do exist sometimes known classes D and E. Class D addresses are used by multicast groups receiving data from applications or server services, and class E is an experimental class not accessible by the public. <heading>b) </heading>To determine the IP address of a Windows based system you need to ensure that you are either connected to the Internet or a network.otherwise the following commands will not provide positive results. Firstly it is necessary to invoke a command prompt window by clicking on start from the desktop, then programs>accessories>command prompt. From here typing 'ipconfig' should bring up details of the computer's IP address. Based on the first byte of the address, you will be able to determine the class of the address as for class A it will be 0-127, class B 128-191, and class C 192-233. If operating on a Linux system the method of determining the machine's IP address is similar. Assuming you are connected to the Internet or a network, you need to invoke a terminal window (the method may vary depending on the flavour of Linux) and type 'ifconfig'. This will display details in a similar manor to Windows and from the address in the same way as before, you can determine the class of the address. <heading>c) </heading>Class C addresses allow for over 2 million network addresses each of which only allow for 254 host addresses. These are perfect for small company or home networks where it is unlikely that you will have more than 254 nodes but for the average sized company these are too small. They are advantageous over class B or A networks which allow for many more hosts because they don't come with the added complication of sub netting. Sub netting is essential on class A or B networks as the vast number of hosts supported is too hard to manage by routers alone. However, class B networks are better than class C networks when there is a large organisation as they allow for many more nodes to be available. This will need to be sub netted but it can be useful when the organisation has many departments as traffic can be kept local to each sub net when two nodes within the same department want to communicate. This will allow for a higher performance on the network. As C class networks are so small, they are much more manageable as there are not so many nodes hanging of the routers and therefore there is not as much traffic flowing around the network. Larger networks that are class A, and B are very hard to manage due to the enormous scale of them and the many routers and subnets involved with extensive routing tables which would slow down the data transmission. However class C networks do carry the disadvantage that they are not as easily scalable as Class A and B because you are only limited to 254 nodes per network address and you would need to introduce another network address to cope if your business expanded leading to several class C networks connected together which again is very hard to manage. If you started with a class B network address you are preparing for growth within your business as these are equipped for 65,000 hosts which is plenty for any organisation. <heading>4) </heading><heading>a) </heading>In terms of computer security, authentication is the verification of a user's identity when accessing a system normally via the validation of a username and password against a database. Authentication should occur on any network so that the system is sure that the person logging on is who they say they are, and it is necessary to protect critical or confidential data that may reside in a user's account. Access control is a method of restricting the availability of resources based on certain privileges that are granted by a higher authority to a particular user. Access control allows for definitions of what users can and cannot do whether it be editing a document, viewing certain tables in a database or changing the wallpaper on a network node. Access control is strongly linked with authentication as authentication can define privileges. <heading>b) </heading>As a systems manager of a large company it is necessary to have strict policies regarding the passwords used for authentication as usually they protect critical information. It is wise to have a password which consists of numbers, punctuation, and letters of different cases; the greater the mixture the better, as passwords that are words can be cracked. This is a good policy because a random string of numbers and letters cannot be guessed but on the downside it is hard to remember a password of this kind. You need to have a password that is easy to remember but nothing that can be guessed such as your favourite band or colour. A good technique is to take the first letter of each word in a sentence such as a song lyric and use this as your password. It is easy to remember, can be worked out again if you forget it and is also very hard to guess. It is a good idea to change your password regularly in case it has fallen into the wrong hands; once a week is probably optimal. This also entails issues concerning remembering and you may be tempted to write the passwords down which is a con with this policy but you can devise a system for changing your password based on numerical schemes e.g. using lottery numbers to correspond to letters in the alphabet. Lastly, ensure that nobody is looking when you enter your password because people can recognise keystrokes and decipher the password. It is good to not look at the keyboard when typing as this aids people in viewing the word. You would need to be able to touch type the word that is a downside but it is a good method because it protects against people finding out your password, which you previously thought was unable to be guessed. <heading>c) </heading>First of all you need to create four files called 'names', email', mobile numbers', and 'sales records' and separate the data into each according to the name of the file. Secondly three groups need to be created called 'Team A', 'Team B', and 'Team C'. The root user needs to allow group members view the files based on the GID of their group and this is done by the permissions the files have and the setgid bit. If a file has a Setgid bit it will run with the privileges of the files group owner, instead of the privileges of the file. Chmod g+rs filename The command above should be given for each file so that when a member of a group tries to access the file they only have the permissions of the group they are in which maybe mean that they cannot read the file at all. <heading>d) </heading>It is clear that usernames and password are not secure enough these days especially when protecting highly confidential information as human memory fails when long passwords consisting of letters and numbers are assigned, passwords can be stolen if written down, or key-stroke software can record the keys typed at login. To obtain higher levels of authentication that is almost fault proof further, more sophisticated measures need to be taken. Companies have introduced schemes whereby you need a swipe card that only works once or between certain times which is combined with a known password so that without either nothing authentication will not be valid. However this has not taken off because companies such as Dell don't fit card readers as standard on their PCs. Using biological characteristics are the most secure method of ensuring authentication as certain parts of the human body are unique. For example, fingerprints can be read via a scanner which is attached to a PC through the USB interface which would almost guarantee authentication, except moulds of fingerprints can be made to get around this system. As fingerprint scanners are falling in price this is a realistic measure for the near future. Retinal scans can be used also in a similar manner and these can not be recreated in any way but devices for these aren't cheap and would only be in place to protect governmental level data. Combining as many different authentication techniques such as a username, password, key card and retinal scan would be the most reliable way to authenticate people. 