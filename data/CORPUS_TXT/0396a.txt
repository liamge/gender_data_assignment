<heading>Part 1: Principal Component Analysis</heading><table/>Note in red that the variance of V3 (liver weight) seems to be significantly larger, as well as its covariance with other organ weights. This is evidence that the scale of variation in the data might be different, largely explained by the age factor of hamsters. <table/>Now both the sample variance and standard deviation are of a similar scale (<1). The log-scale, particularly of base 2 in this case, helps a lot mainly because the growth of hamsters is achieved primarily by every cell dividing into two, i.e. an exponential growth of base 2. So the log-scale can reduce a great deal of variation due to the age factor and hopefully bring the data to a comparable scale. But there can still be other factors that are left unaccounted for. In addition, scaling can be quite subjective and had a different scaling been adopted, the result would have been different. <table/>So the first two principal components explain about 84.9% of the total variation in the data. <table/>The first component has all positive entries, so it can be regarded as a size vector. Large values along this direction in the graph would mean a general large size of all organs. The second component displays the contrast between V2 (heart) and V6 (testes). The actual direction of axes could be different from what SPLUS command princomp() produces. But in this case, a positive value along the direction of the second component means relatively heavy testes compared to heart and vice versa. It is interesting to observe the slight variation between what SPLUS princomp() produces and what the real eigenvalues and eigenvectors are. <table/>Three things that seem to be observed about the SPLUS command princomp() are: It centres the data automatically The directions of the new axes are automatically decided although they could be set in other directions. Loadings are selected to reflect their relative importance, i.e. some small and non- influential values are set as zero. <quote>> plot(Y$scores[, 1], Y$scores[, 2])</quote>The plot is the 'best' 2-D summary of the data in the sense that the 2-D plane captures 84.9% of the variation in the data, i.e. it displays a 2-D slice of the 6-D data with the largest area covered. <figure/><heading>Part 2a: Data Recovery from dij - Euclidean Distances</heading><quote>> HX <- X > for(k in 1:6) HX[, k] <- HX[, k] - mean(HX[, k]) > Q <- HX %*% t(HX)</quote>The Q matrix is just the sample variance matrix and hence symmetrical. So one way of recovering the centred data matrix HX is to use metric scaling - orientating the recovered HX in the same way as PCA does. <quote>P <- eigen(Q)$vectors D.half <- diag(sqrt(eigen(Q)$values)) HX1 <- P %*% D.half</quote>In theory, Q here should be positive semi-definite with all its eigenvalues non-negative. However in this case, some eigenvalues are computed by SPLUS to be negative but very close to 0. So this causes a little problem when producing HX1 and its covariance matrix - there are many NA values. <quote>> a <- rep(0, 73) > for(i in 1:73) a[i] <- var(HX1[, i], na.method = "include") > (a[1] + a[2])/sum(a, na.rm = T) [1] 0.8488463</quote>By treating the NA values as just 0's, I can still find the proportion of total variation in the data explained by the first two columns of HX1 to be 84.9% - same as PCA result. <figure/>As we can see, the two graphs are identical, but with a 180-degree rotation. This is only because of a different definition for directions of the eigenvectors (axes). Again, it confirms the fact that PCA and metric scaling are related processes. <heading>Part 2b: Data Recovery from dij - another proximity matrix</heading><quote>> d1 <- matrix(0, 73, 73) > for(i in 1:73) for(j in 1:73) for(k in 1:6) d1[i, j] <- d1[i, j] + abs(HX[i, k] - HX[j, k])/(abs(HX[i, k]) + abs(HX[j, k])) > d1.sq <- d1^2 > m <- mean(d1.sq) > Q1 <- matrix(0, 73, 73) > for(i in 1:73) for(j in 1:73) Q1[i, j] <- -0.5 * (d1.sq[i, j] - mean(d1.sq[, j]) - mean(d1.sq[i, ]) + m)</quote><table/>Clearly there are some (far-from-zero) negative eigenvalues which prohibit Q1 from being positive semi-definite by definition. <heading>Part 2c: Data Recovery from dij - a better proximity matrix</heading><quote>> d2.sq <- matrix(0, 73, 73) > for(i in 1:73) for(j in 1:73) for(k in 1:6) d2.sq[i,j] <- d2.sq[i,j] + (abs(HX[i,k] - HX[j,k])/(abs(HX[i,k]) + abs(HX[j,k])))^2 > m <- mean(d2.sq) > Q2 <- matrix(0, 73, 73) > for(i in 1:73) for(j in 1:73) Q2[i, j] <- -0.5 * (d2.sq[i, j] - mean(d2.sq[, j]) - mean(d2.sq[i, ]) + m)</quote>Now the matrix Q2 is positive semi-definite by checking that all its eigenvalues are non- negative. So we can apply metric scaling to obtain a newly recovered HX2. <quote>> P <- eigen(Q2)$vectors > D.half <- diag(sqrt(eigen(Q2)$values)) > HX2 <- P %*% D.half</quote><figure/><figure/><figure/>The new 2-D summary of the data quite clearly shows four clusters of points. And the density plots along both axes just double confirms this special pattern. Clearly some hidden features in the Euclidean distance have been picked up by the new proximity matrix. <heading>Part 2d: Data Recovery from dij - Gaussian sample</heading><quote>> for(k in 1:6) HX[, k] <- rnorm(73) * sqrt(k) > for(k in 1:6) HX[, k] <- HX[, k] - mean(HX[, k]) > Q <- HX %*% t(HX) > P <- eigen(Q)$vectors > D.half <- diag(sqrt(eigen(Q)$values)) > HX1 <- P %*% D.half</quote><quote>> a <- rep(0, 73) > for(i in 1:73) a[i] <- var(HX1[, i], na.method = "include") > (a[1] + a[2])/sum(a, na.rm = T) [1] 0.5917205</quote>This time the first two columns of HX1 only explain 59.2% of variation in the new data sampled from a normal distribution. <figure/>This plot seems to be more evenly spread out than the one obtained from the original data matrix, also with a larger scale on both axes (this time the matrix has not been log-scaled before centering). But the fact that it explains much less variation in the data implies that the original data matrix might be quite far from a truly normal sample. This means that apart from the age factor that has been reduced by the log-scaling, there might still be some linear age factor or other different factors left such that the scaled data is still not quite normally distributed. <quote>> d2.sq <- matrix(0, 73, 73) > for(i in 1:73) for(j in 1:73) for(k in 1:6) d2.sq[i,j] <- d2.sq[i,j] + (abs(HX[i,k] - HX[j,k])/(abs(HX[i,k]) + abs(HX[j,k])))^2 > m <- mean(d2.sq) > Q2 <- matrix(0, 73, 73) > for(i in 1:73) for(j in 1:73) Q2[i, j] <- -0.5 * (d2.sq[i, j] - mean(d2.sq[, j]) - mean(d2.sq[i, ]) + m) > P <- eigen(Q2)$vectors > D.half <- diag(sqrt(eigen(Q2)$values)) > HX2 <- P %*% D.half</quote><figure/><figure/><figure/>Compared with the previous results, there is much less clustering in the 2-D summary with a normal sample. The two peaks displayed in the density plot of HX2[, 2] (above right) is only a special case. Once the random sampling is repeated, I found that it does not normally show this kind of peaks. Overall, the plots are suggesting that a truly normally sampled data matrix shouldn't display unusual clustering that our data shows. It is interesting to see that using the Euclidean distances doesn't extract this kind of pattern as clearly as the 'better-defined' proximity matrix does. So the metric scaling with respect to (*) in the question sheet somehow enlarges the hidden factors that are left out after log-scaling and are causing the sample to be non-normal. My understanding is that d ij is: <list>very close to zero if hamster i and j are both grown-up hamsters or both baby hamstersvery close to 1 if i is a grown-up and j is a baby hamster or the other way round.</list>This essentially causes the grouping / clustering of grown-up and baby hamsters in the 2-D summary plot. And therefore this 'better' proximity matrix is able to pick up the hidden age factor that is still left in the data after log-scaling but is not detectable using the normal Euclidean metric scaling. 