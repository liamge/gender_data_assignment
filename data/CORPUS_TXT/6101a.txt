<abstract><heading>Abstract</heading>This paper serves as a project report for an assignment into image compression. The task was to compress 256-pixel square greyscale images, with lossless techniques or not, into a smaller file. I achieved the aim using a combination of run length encoding, reducing the number of greys in the image, and applying the Huffman technique. This report documents this process, and contains the results. </abstract><heading>Introduction</heading>The brief was to take a 256-pixel square greyscale image, for example Fig. 1, and compress them as much as possible without loosing too much information about the face. <picture/><heading>Method</heading>I will split the method into the three different compression techniques. They are applied in the order they are given during compression, and, obviously, in reverse for decompression. <heading>Grey Level Decrease</heading>Because I was aiming to write a compression algorithm that would encode and decode pictures of faces, I decided that the background was of little importance relative to the face itself. By experimentation, I found that 32 levels of grey were all that were needed to represent the face in a reasonable amount of quality. The main negative effect of reducing the colour depth is that gradients become stepped. As faces usually do not contain many gradients, the only place this should be noticeable is in the background, which shouldn't matter so much. Please refer to Fig. 2 for the same image, but represented with 32 greys. <picture/><heading>Run Length Encoding</heading>Fig. 3, below, shows a detail of this image taken from the top left corner: <picture/>As you can see, there are very large patches of the same level of grey. In this case, it is very wasteful to represent every pixel with an 8-bit number (or 6-bit for 32 grey levels), when we can describe the patches themselves. This idea gives several options, most notably run length and quad tree encoding. The na√Øve view of quad tree encoding is that the square regions of the same colour get boxed together and labelled as one entity in the tree. In fact, of course, this is not the case. The boundaries of each region are set arbitrarily by cutting the image recursively into quadrants. Therefore, an area of constant colour could be described by many regions, if the cuttings do not happen to fall in the best places, and the solution will not be optimal. Fig. 4 is a small detail of Fig. 2 taken at random from the top right of the left eye. <picture/>This is typical of the face part of the image in 32 greys (the background is generally more patchy, but more of the images are face than background). There are definite regions of constant shade, but none of them particularly large. If you apply quad tree encoding to this, you get the following: <picture/>As the regions of grey are not conveniently placed and sized, and as they are not particularly large, quad tree encoding, as can readily be seen, has little effect. In the example of the region above, there would be 233 leaf nodes to the tree. This means that there will be 233 8-bit numbers to be stored, as well as the tokens in the non-leaf nodes. <picture/>Fig. 6 shows the boundaries given by run length encoding. There are 87 regions in this example, and for each of these two items need to be stored, the intensity and length of the region. Each of these can be stored in an 8-bit number; so 174 values will classify this image. As this example shows, where the regions of colour are reasonably large but not very large, run length encoding is better than quad tree. <heading>Huffman Encoding </heading><figure/>The histogram of Fig. 7 shows that some levels of grey are dramatically more frequent than others. In fact, the standard deviation of the level frequencies is, in this example, 53.0. If the five grey levels that make the central peak were all described with short codes, and the other grey levels with longer codes, the result will obviously be more efficient. This fact applies even more to the lengths of the codes. The longest run in this particular image is 76 pixels, but the vast majority, as can be seen by the following histogram, are under 10. Even if there were an encoding method did nothing other than allowing one bit for a run length of 1, and an 8-bit code starting with a zero for everything else, there would still be 99,400 bits saved. <figure/>To compress the image further, therefore, the Huffman technique was employed. The pseudo-code for the implementation of Huffman encoding used to encode a string of binary values (in 8 bit sections) is as follows: <list>Split the input string into strings of 8 bitFind the frequency of each stringPut a frequency table at the start of the output string. This is made of 256 16-bit binary numbers.Initialise a forest of 256 trees. Each of them will have a value between 0 and 255, and the frequency of that value as a weight.Repeat until there is only one tree:Find the two trees with minimum weightsCreate a new tree, and put these two trees as its children. Assign a weight to this new parent of the sum of the children's weights.Reduce the children's weights to zeroFor every 8-bit string in the input:Find the node in the tree with the string's value. Let's call it CurrentNode.Create a temporary stringIf CurrentNode is the left-hand child of its parent, add a 0 to the temporary stream, otherwise add a 1.Repeat until we are at the root:Reassign CurrentNode to its parentIf the new CurrentNode is the left-hand child of its parent, add a 0 to the temporary stream, otherwise add a 1.Reverse the temporary streamAppend the temporary stream to the output stream</list>When it is decoded, the tree is built from the frequency table at the start of the string, using the same method as steps 4 and 5 above. As there is no random element to either process, the tree will always be the same. After the tree is built, the string (without the frequency table) can be decoded with: <list>Repeat until the end of the input string has been reached:Create a variable called CurrentNode. Let it point to the root.Repeat until we reach a leaf node:If the next character of the input string is 0 then reassign CurrentNode to its left childIf the next character of the input string is 1 then reassign CurrentNode to its right childAdvance our position in the input string by one characterPut the next number on the output stream as the value of the leaf.</list>One idea for Huffman encoding was to encode the run lengths and intensities separately, so that each would have its own frequency table. The reasoning for this was that the frequencies would be more accurate if they were separate. After further thought, it can be seen that that idea is wrong. The frequencies of the data are absolute, no matter what they represent. That idea would have just resulted in an extra frequency table. <heading>Results</heading>For the results of each technique, I shall test them on a single image. Obviously, exact compression ratios vary image to image, so I shall give a set of compression ratios for different images in the Overall Compression section. For the purposes of examining the techniques individually, it is necessary to give their results individually. This, however, causes a problem as the compression achieved by each of the techniques is dependent on the compression that has already been achieved. I will try to break it down as much as possible, however. <heading>Grey Levels</heading>This is the simplest to evaluate. Given a 256 square image, there will be 65,536 pixels. If each of them were assigned a 6-bit binary number (0 - 31) then this would take 393,216 bits, or 49.152kb. In this application, the reduction of grey levels does not have this effect quite though, as, for the convenience of Huffman, they are each encoded in 8 bits. Huffman then compensates for this inefficiency by assigning longer codes for the values over 32, which are never used, and short codes for values under 32. <heading>Run Length</heading>When the image has 256 levels of grey, run length encoding "compresses" the image up to 77.484kb. This is because with so many levels of grey, there are very few runs. When the grey levels have been reduced to 32, run length encoding (while still assigning 8 bits to both the intensity and length of each run) compresses the image down to 27.029kb. <heading>Huffman Encoding</heading>When the image has 256 levels of grey with no run length encoding, Huffman compresses the image down to 54.708kb. When the image has 32 levels of grey (each represented with 8 bits), and no run length encoding, Huffman compresses the image down to 31.941kb. <heading>Overall Compression</heading>When all three techniques are applied together, the image is compressed down to 14.480kb. Here is an overall comparison of the three techniques: <table/>Actually, you can divide each of those percentages by three, as the original image had a red, green and blue stream, each of which stored the intensity of the grey. This brought the original file size up to 196.608kb. This means that the final compression ratio of [Original Image : Compressed Image] is 0.0736523 : 1. Alternatively, that could be expressed as 1 : 13.5773085. The compression time is 0.125 seconds. The decompression time is 0.101 seconds. All the statistics above are based on a single image, however. Here are the compression ratios (as a percentage of the original 196.608kb file) for the 8 test images: <table/>It is interesting to note that the subjects of tests 7 and 8 were black, so this perhaps indicates that an image of a black person is compressed better using this technique than that of a white person. This is either an indication that there is less variety of tone in a black person's skin, or that the lighting conditions and camera were such that less detail was picked up. Figs. 9 and 10 (overleaf) show an example image before and after compression. <picture/><picture/><heading>Comparison With Other Techniques</heading>I shall compare my typical compression ratio with those of other techniques. First, I will (quite unfairly) compare it to lossless techniques. <table/><heading>Future Development</heading><heading>Conclusions</heading>If one were to put an original image next to an image that has been compressed then decompressed under my technique, the only difference that would be noticeable would be the stepped gradient in the background, as opposed to the gradual gradient of the original. As has been discussed before, the background is not of huge concern, so therefore the compressed images are valid. The compression ratio of between 7 and 13% is very good, and a computing time of between 0.1 and 0.2 seconds for both compression and decompression is negligible. The compression ratio is respectable, relative to the well-used standard techniques. It is obviously not as good as the JPEG standards, but that is far more complex than mine. I have no statistics to this effect, but I would guess my technique to be faster than some of the others that achieve better compression ratios. As 1 : 13 is a good rate of compression without losing psychovisually important information of the face, and as the compression time is very small, I therefore deem the project to be a success. 