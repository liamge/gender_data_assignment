<abstract><heading>Abstract</heading>Nowadays, a lot of scientific research domains (such as medicine, cancer researches, aerospace, etc...) use some applications solving several complex mathematical algorithms. Then, these applications need a lot of computational resources to be executed. The research in computing developed some solutions supplying a lot of computational resources such as distributed computer (cluster), parallel execution, etc... Hence, these solutions can be used to execute complex scientific applications which need a lot of resources. These applications would then be executed in a parallel way. The main purpose of this project has been to develop a "school scientific" application using the  distributed computer called Beowulf. This application, developed in C using MPI, computes the matrices multiplication using two ways (sequential one and parallel one) and solves a system of linear equations (as well as the inverse of a matrix) using the iterative Jacobi method. This application enables us to understand why and in which conditions a parallel algorithm is often better than a sequential one, explaining these two ways and calculating the efficiency of the parallel solution. Hence, this project involves a part of researches dealing with computing technologies (such as distributed computing, parallel execution, etc...) as well as mathematical concepts using matrices and system of linear equations. Another main part of this report deals with the mathematical application which has been created explaining how such mathematical algorithms have been developed in C using MPI. <quote>Résumé De nos jours, la recherché dans beaucoup de domaines scientifique (tel que la médicine, la recherche contre le cancer, l'aérospatial, etc...) utilisent des applications qui résolvent des algorithmes mathématiques complexes et qui nécessitent énormément de ressources de calcul. La recherche en Informatique a permis de mettre au point des solutions fournissant de telles ressources, tels que les grappes de PC (cluster), les exécutions parallèles, etc... Ainsi, ces solutions peuvent être utilisées afin de permettre l'exécution d'applications scientifiques complexes qui demandent énormément de ressources de calcul. Ces applications peuvent ainsi être exécutées de manière parallèle. Le principal objectif de ce projet a été de développer une application scientifique "école" utilisant la grappe de PC, nommée Beowulf, appartenant à l'. Cette application, qui a été développé en C utilisant la librairie MPI, a pour but de calculer des produits de matrices utilisant deux méthodes (une méthode séquentielle et une autre parallèle) et de résoudre des systèmes d'équations linéaires en utilisant la méthode itérative de Jacobi. Grâce à cette application, nous pourrons comprendre pourquoi et dans quels cas un algorithme parallèle est préfér éà un algorithme séquentiel, ceci en expliquant les deux méthodes et en calculant l'efficacité de la solution parallèle. Ce projet a impliqué une importante phase de recherche concernant les technologies Informatiques utilisées pour exécuter des applications parallèles (tel que des grappes de PC (cluster), les exécutions parallèles, etc...) ainsi que certains concepts mathématiques concernant les matrices et les systèmes d'équations linéaires. Une autre partie importante de ce rapport explique comment l'application mathématique développée au sein de ce projet a été créée en langage C utilisant la librairie MPI.</quote></abstract><heading>Introduction:</heading><heading>Project presentation:</heading>Nowadays a lot of scientific domains (especially medicine, aerospace, fluid mechanic (e.g.: CFD), physics, chemistry, etc...) need more and more computational power. Indeed some scientific calculations need a long time to be executed, even on a powerful computer (sometimes several weeks or months). For example, such applications could use a lot of complicated formulas using a floating point which need a high precision or some genetic algorithms are used which need a long time to go throughout the generations, etc... That is why several research groups throughout the world are developing some solutions. These solutions are getting more and more efficient and allow the scientific calculation to be executed quickly. In this project, we will present and use some technologies (e.g.: MPI, Cluster, distributed computing, ...) developed for the purpose of providing a lot of calculation power for scientific application. Through some typical mathematical applications such as multiplication of matrices (with large matrices) and solutions to systems of linear equations (using the iterative Jacobi method, solving the inverse of a matrix), we will explain how these technologies work and how we can use them. Hence, this project involves the development of a mathematical application using distributed computing. <heading>Why did I choose this project? </heading>The first time I heard about distributed computing used to solve very complex applications executing on several processors, I was very interested and intrigued and I wanted to know more about this subject. Through this project, I decided then to discover "this world" of distributed computing. Furthermore, Brookes University has its own cluster named Beowulf, so it was a great opportunity to be able to test concretely such applications on a real multi-computers system. For me, this project involves a lot of new concepts and technologies which I knew a little bit before, but I had never used them practically. I really wanted to know more about it, and I had the opportunity to use it concretely thanks to the Brookes University cluster. <heading>Body of this report:</heading>This project involved a long part of theory, because some computing and mathematic concepts needed to be understood before creating an application using these concepts. That is why this report contains a long part dealing with some theoretical concepts and another main part dealing with the produced application. First of all, we will introduce in details the technologies that we will use to develop a mathematical application and we will then see the reasons why a parallel solution is often better than a sequential one. After that we will present the mathematical issues and explain how it is possible to compute them in a sequential way and in a parallel way via some algorithms given in a pseudo-code. Eventually, we will explain in details the produced solution developed through this project, which solves the mathematical issues explained before. <heading>Technologies involved by a parallel and distributed application:</heading><quote>"From the highest to the humblest tasks, all are of equal honour; all have their part to play", Winston Churchill</quote><fnote>From [Bib B] </fnote>In this part, we will explain in a first time what is exactly a parallel execution, the reasons why we prefer to use it rather than a sequential execution (in some case). Then we will introduce the "hardware" environment in which such parallel applications can be used. Afterwards we will explain how it is possible to create, run and manage such applications in this environment. After that, we will be able to explain which technological solution has been used in this project. <heading>Parallel application (parallelism):</heading><heading>Generalities:</heading>A sequential execution executes several instructions one after another. As it is called "sequential", it is simply a sequence of some instructions. However, a lot of scientific applications need a lot of calculation time, that means they need a lot of calculation resources (basically CPU). Hence, in order to satisfy these resources requests, the parallelism notion is introduced to enable such application to be executed faster. In the general case, the parallelism is the division in several parts (partially or totally independent) of the program. There are two different kind of parallelism: the intern parallelism and the extern one. In both, the initial program is cut in several parts, The intern parallelism: this kind of parallelism is typically the one which is used on a monoprocessor machine. Actually every parts of the program would be executed on a dedicated process or thread. Then the operating system will schedule the execution time of each part. So, when a part is running, then the others are waiting for the end of this one, and every process will be executed one after another, until every process gets fully executed. The extern parallelism: this kind of parallelism is more efficient than the previous one. Indeed, this parallelism is performed either on a multiprocessor computer (typically a supercomputer) or on a cluster (a distributed system, for example Beowulf) or even on a grid computing. Each part of the parallel program will be executed on a dedicated processor. Hence, each part will have its own processor and so will be enabled to use the whole power of this processor. <fnote> See section 2.3. </fnote><fnote> See section 2.3. </fnote><fnote> See section 2.3. </fnote><fnote>Depending on the platform architecture (cluster or supercomputer), the memory will be shared between the part or not. </fnote>However, each part comes from the same program. So they will have to be synchronized when each one will be executed. In order to do that, a procedure called Rendez-Vous has to be done. We also really have to be careful about the mutual exclusion. Indeed, if several parts want to use a global variable (which is declared in the main program), a mutual exclusion system should be used. The schema below shows that how a parallel execution is performed, <figure/>As we can see on the schema (Figure 1.1.), the program is executed by only one process until the program gets separated in two different processes which are executed at the same time (parallel execution). Then, these two processes have to be synchronized. So, when we want to use a parallel execution, the programmer has to find a way to parallelise a sequential algorithm. Then we will be able to calculate the performance gain by using n processors in parallel instead of only one in a sequential way. In order to create and to use such parallel applications, we have to use some libraries which allow and manage the parallelism. The two most popular are MPI and PVM. <fnote>Message Passing Interface </fnote><fnote>Parallel Virtual Machine </fnote><heading>Parallel Virtual Machine (PVM):</heading>PVM is a tool which managing the parallelism on a distributed platform (cluster). This tool works as a daemon. Indeed, to execute a parallel program which has been developed using some PVM methods, a PVM daemon has to be running on every nodes of the cluster. To start this daemon on the different available nodes, the following command is used,  FORMULA  Where hostfile is a simple text file which contains the name of all the nodes needed and capable of be used An example of a hostfile could be as follows,  FORMULA  Then the daemon PVM (pvmd) would be launched on all these machines (except if one of them is down) in the background (daemon). It is possible to have an access to the PVM console in order to administer PVM using several commands which show the current machines running the pvm daemon, allow adding some machines in the virtual machine, showing the different jobs which are running, etc... To launch the PVM console, the following command can be used,  FORMULA  Above is a short description of PVM, but this tool won't be used in this project. It has been presented because of its popularity in the "distributed computing world". Actually another tool called MPI had been used in this project. <heading>Message Passing Interface (MPI):</heading>MPI is a set of rules which define the way to send some messages between each process executing a parallel program. This library is known to be portable; hence it can be used on almost every kind of distributed system. However in order to use MPI routines, a MPI implementation (for example LAM/MPI, OpenMP, ...) has to be installed on the cluster. These implementations are free for almost all of them. In this project, MPI will be used to perform the parallelism, using the implementation LAM-MPI which is already installed on the Brookes University Beowulf cluster. <fnote>Local Area Multicomputer </fnote>Before being able to launch a parallel program using MPI on the Beowulf cluster, the tool LAM-MPI has to be started on every node available which will be used. The following command line manages it,  FORMULA  Where '-v' means 'verbose' to show the result of this command and hostfile is a text file which contains the name of every nodes available (similar file as the hostfile in the section 1.1.1. used by PVM). It is possible to stop LAM by the following command,  FORMULA  LAM deals with the messages sent by the processes to each others. It also manages the parallelism. To compile a program using MPI, the compiler called mpicc is used,  FORMULA  When LAM is running, we can launch a parallel program which has been, of course, developed using MPI methods. To execute a parallel program, the following command is used,  FORMULA  Where '-np' means "number of processors", n is the number of processors needed and Application is the parallel program which we want to execute. Then, the parallel application is executed and the standard output/input are used to print and read data. <heading>Performance analysis:</heading><heading>Speedup:</heading>In general, as parallel algorithm is used in order to improve the execution performance. So, it is useful to calculate something which could tell us the benefit we gain to use such an algorithm instead of a sequential one. The speedup is a number which determines how much the parallel algorithm is better (faster) than the sequential one. The speedup is determined as follows,  FORMULA  Where  FORMULA  is the speedup for p processors,  FORMULA  is the execution time of the sequential algorithm (so, on only 1 processor) and  FORMULA  is the execution time of the parallel algorithm on p processors. As it is natural to think,  FORMULA  would be always greater than 1. Otherwise, that would mean the parallel algorithm is less fast than the sequential one, and if this is the case, it is really not recommended to use a parallel algorithm. In the opposite way, we can see that the ideal value which can be obtained is p. In this case, we have a linear speedup. <heading>Amdahl's law:</heading>It is important that to know the speedup can be determined by a law stated by Amdahl. However, this law won't be used for this project but it is important to speak about it. <fnote>Gene Myron Amdahl was a computer architect during the 50's </fnote>This law is stated on the thought which says that the speedup has to be determined by the algorithm and not only by the number of processors. Basically, a sequential algorithm can be improved by parallelising the code. However, it is possible not to be able to parallelise the algorithm anymore. So the speedup has to be determined knowing this point. Hence, only a defined part of the algorithm will be parallelised, and the other part will stayed sequential. For example, only 40% of an algorithm might be parallelisable. This law is stated as follows,  FORMULA  Where  FORMULA  is the speedup for p processors and f is the percentage of the parallelisable part of the algorithm. In this project, we won't use this law. Indeed, the speedup will be calculated experimentally. <heading>Efficiency:</heading>The efficiency is simply a number (in general between 0 and 1) which determines how well the processors are used when they are executing the parallel algorithm. The value is determined as follows,  FORMULA  Where  FORMULA  is the speedup for p processors, P is the number of processors and  FORMULA  is the efficiency for p processors. If the speedup is linear, then the efficiency is equal to 1. These values are very useful to determine whether or not the parallel algorithm is more efficient than the sequential one. <heading>Parallel or sequential algorithm? </heading>For a given problem, the best way to compute it has to be chosen. In fact, in some case the sequential algorithm is more efficient than the parallel one. It is natural to wonder why. Indeed, if we have n processors, we naturally think that the algorithm will be executed faster than with only 1 processor. But it's not necessary true. Actually, some external factors modify the thought which are the communication between the processes using a local area network (so, some physics factor could modify the data transfer time), the synchronisation between the processes etc... Hence, it is important to calculate the speedup and the efficiency in order to know whether a parallel solution is efficient or not, for a given problem. <heading>Different environment available to execute a parallel application:</heading>As we said before, a mathematical application needs a lot of calculation resources. So, such applications are often parallel and each part of the program is executed on a different processor. So, we need a platform which contains several processors to run this kind of application. Actually several platform could be used. Two kinds of environments exist: a multiprocessor supercomputer (multiprocessors computer) and a cluster. <fnote>A list of the 500 most powerful supercomputers in the world is available on the website  URL  </fnote><heading>Multiprocessor supercomputer:</heading>A super-computer is a single computer which contains several processors (multiprocessors). That means each processors has its own cache memory (of course, because the cache memory is include in the processors) but the RAM is shared among each processors. It could be a drawback because some access right has to be managed. Furthermore such a computer needs to have a large capacity of RAM, so it gets very expensive. A famous example of a multiprocessor supercomputer is the Earth Simulator located in Japan. <fnote>Random Access Memory </fnote>Below is a picture (Figure 1.2.) of this supercomputer, <picture/><heading>Cluster:</heading>A cluster is another kind of supercomputer. Actually this is a set of personal computer not necessary very powerful. All these computers are bound via a very high speed network (a LAN) in order to manage the communication between all of them. So, the power of a cluster comes from the union of these computers. Each computer in the cluster is called "a node". As each node is a single computer, each processor has its own memory. So the RAM is not shared like in a multiprocessor supercomputer. Hence, it is consequently a cheaper solution. Each node being a single personal computer not necessary expensive, this solution can be created for a smaller cost than the multiprocessor solution. <fnote>Local Area Network </fnote>Among the nodes is a root node which allows the users to get connected on the cluster. Furthermore this root node allows an administrator to configure the cluster and is also a data server. The root node is linked to an extern network (usually Internet) to enable the users to get connected via an extern connection. Brookes University has its own cluster called Beowulf . Each node is called Beowulfn where n is the number of the node (0≤n<9). The root node is Beowulf0. <fnote>The name Beowulf comes from the famous hero of the epic poems </fnote>A schema (Figure 1.3.) explains how the general architecture of a cluster is, using the Brookes University Beowulf as an example, <figure/>This picture (Figure 1.4.) is the Beowulf cluster at Brookes University, <picture/>On of the main advantage of a cluster is the transparency for the users. That means when a user uses the cluster, he doesn't need to know how is structured the cluster, which node is currently running or not etc... Through a remote connection (via a Unix console), the cluster seems to be only one single computer for the user. Then, when the user wants to run a parallel application, he just has to launch it without thinking about the parallelism, the application will be executed on the different node and the result would be available for the user as if it was execute only on one computer. Another kind of environment with another scale dealing with grid computing can also execute parallel applications. To have more details about this environment, see appendix 1. <heading>From the mathematical issues to the algorithmic solutions:</heading>As this project involves some mathematical concept, we have to introduce them before explaining how compute them. So, in this part the main mathematic concepts concerning matrices are explained in order to introduce the problem of systems of linear equations. Hence, the principles of matrices will be described as well as different methods used to solve a system of linear equations. After that, the first step to compute such mathematical problems will be shown via some sequential algorithms given in pseudo-code. These algorithms will be explained and the way to parallelise such algorithms will be described. <heading>Prerequisite mathematical concepts:</heading><heading>Matrices:</heading>The problem which we will try to compute in this project will use some matrices. Matrices are a very important and useful mathematical element in computing, especially in numerical computing. In fact, a matrix is simply an array (in general 2 dimension but can be more) of scalars. Here is the general for of a matrix,  FORMULA  We can also write,  FORMULA  If n=m, then the matrix is called square matrix. In the project, only square matrices have been used. <heading>Matrices multiplication:</heading>A matrices addition and subtraction are very simple. If A is a matrix and B another one, A has to have the same size as B, otherwise the addition/subtraction is impossible. Then, every element of the matrix A are added (or subtracted) to every element of the matrix B which is "at the same place". Mathematically, that means, Let A and B be two matrices with the size n x n such as,  FORMULA  Then, the addition of A and B is,  FORMULA  As we can see, this operation is very simple. The addition is commutative. The multiplication is a bit more complex but very natural. A precondition concerning the matrices dimensions has to be true to be allowed to calculate the product. To multiply the matrix A (m x n) by B (u x v), then n has to be equal to u, otherwise the product doesn't exist. The product is expressed as follows, Let A be a matrix m x n and B be a matrix u x v, then the product A by B is,  FORMULA  Unlike the addition, the multiplication is not commutative. That means the product  FORMULA  is different than the product  FORMULA . It is even possible that AxB exists but BxA doesn't exist; in this case, A and  B are not square, otherwise A and B are two square matrices. In the appendix 2, is an illustrated example. Even if it seems very trial, this example can help a lot to understand the way to compute a parallel algorithm doing this operation. In fact, as we will see in another part, the "visual understanding" will help a lot to find a way to create a parallel algorithm. In the general case, with A and B two square matrices of dimension n, the number of simple operations Nb needed is,  FORMULA  The detail of the equation is shown (via a concrete example) in the appendix 2). This function Nb(n) is cube, that means more the number n is great, more Nb is great. Furthermore, the cube function increases very quickly, so the number of operations needed gets very great with a great n. Below is the graph of the function (Figure 2.1.), <figure/>If we use a computed sequential solution to calculate the product of two matrices, we can notice that the computer will have to do a lot of operation for n very great. So, a sequential solution "will stay a long time in a loop operation" and so this would take a lot of computational resources. That's why a parallel solution will be done. However, more details will be given in another part of this document. <heading>Systems of linear equations:</heading>In this part, we will explain what a system of linear equations is mathematically; afterwards we will present the Jacobi method which is used to solve such systems. <heading>What is a system of linear equations? </heading>A system of linear equations is a set of linear equation containing several unknown variables. In the general case, such a system can be mathematically expressed as follows,  FORMULA  As it is easy to think, we can write such a system using matrices as below,  FORMULA  Where A is the matrix containing all the coefficients, B is a column matrix containing the results of each equations and eventually X is also a column matrix which contains all the unknown variables. Hence, using the matrices notation, the problem of system of linear equation is to solve the following equation and then to find the unknown variable (vector X),  FORMULA  When n is not too great, it is possible to find the solution solving the system "by hand", which means without specific methods except using back substitution, but this kind of resolution gets very problematical and often need a long time. Furthermore, such resolution without formal method is very complex and not efficient to compute. However some more efficient methods have been found such as Gauss-Jordan method, the Gaussian elimination method, the iterative Jacobi method etc... In the next part is explained one of them which is called the Jacobi method. <fnote>The Gaussian method is explained in Bib[D] </fnote><fnote>"The method is named after German mathematician Carl Gustav Jakob Jacobi.",adapted from the Wikipedia Encyclopedia </fnote><heading>The Jacobi method:</heading><heading>Jacobi method:</heading>The Jacobi method [Bib A] is a method solving a system of linear equation AX=B. This method is iterative, that means the solution would be approached to a final solution after some iterations. The system will converge forward a final solution but will never reach it (actually, a determined tolerance value has to be used as a threshold to stop the iteration). Hence a solution would be found after each step (k+1) and if each solution is enough closed from the last one (that would mean the tolerance convergence is reached), then the vector is saved as the final solutions vector and the algorithm finishes. Before explaining mathematically how this method works, some little definitions has to be defined, - Diagonal matrix: Such a matrix has non null element only on its diagonal. So, this kind of matrices can be defined as follows,  FORMULA  - Strictly Lower triangular matrix: Such a matrix has non null element only on the lower part of the matrix (under the diagonal). So, this kind of matrices can be defined as follows,  FORMULA  - Upper triangular matrix: Such a matrix has non null element only on the upper part of the matrix (above the diagonal). So, this kind of matrices can be defined as follows,  FORMULA  As it is natural to think, we can write M = L + D + U where M is a matrix, D its diagonal, U its upper matrix and L its lower matrix. Knowing these definitions, we can define the Jacobi method. We want to solve AX=B with X the vector of unknown variables,  FORMULA  Hence, we have the definition of the Jacobi method which is,  FORMULA  And so, for one row (of the result matrix),  FORMULA  As we can see, this method is a good one to be computed, because it is iterative. When the system converges, we will determine if the convergence is sufficient, that means, whether we are enough closed from the solution or not. The initials values of  FORMULA  for k=0 are guessed. In others words,  FORMULA  will be initialized with the value 1.0 or 0.0. <heading>Condition to stop the algorithm:</heading>The algorithm will be stopped when the last solution will be enough closed from the previous solution. That would mean we are much closed from the "true" solution because it is a convergent iteration. So, we can define  FORMULA  such as,  FORMULA  When  FORMULA  will be smaller than a given value, then the vector  FORMULA  will be the solution vector, because we consider that it is enough closed from the final and "true" solution. <heading>Condition to use the Jacobi method:</heading>The Jacobi method uses a convergence state to find the solution. Thus, the system has to be convergent. If that is not the case, then such a method cannot be used. So, the system has to follow some rules which are define below, For every element of A in AX=B,  FORMULA  However, the matrix might not follow this rule and the system would converge anyway though it is not always the case. In any case, if this previous rule is not followed, it is required that the following condition has to be true,  FORMULA  That means every term belonging to the diagonal must be greater than every other element in the matrix. Nonetheless, if this previous condition is true, the system won't necessary converge, but if this condition is true, that means the system might converge. <heading>Algorithms solving such mathematical issues:</heading>In this part, we will present different algorithms in pseudo-code solving some mathematic operations dealing with matrices and executing the Jacobi method. <heading>Matrices multiplication:</heading>The matrix multiplication has been implemented in this project in two ways, the sequential one and the parallel one. It has been implemented in order to understand how works a parallel program, how manage to run a program on a cluster etc... It is very important because some problems and difficulties have been met and it is easier to control them with a simple mathematic problem. Indeed, knowing these problems, the implementation of a more difficult problem such as a system of linear equations can be done. In fact, if we first start with a difficult mathematical problem and we also have to manage the different computing problems, it could be complicated. So, it is really easier to start with a simple mathematical problem to understand the computing difficulties. Hence, that has been done for this project, and the matrices multiplication using distributed computing is an important part of it. <heading>Sequential algorithm:</heading>The sequential algorithm dealing with matrices multiplication is very simple and straightforward. This algorithm is shown above using a pseudo-code (Figure 2.2.), <figure/>In this algorithm, the multiplication A by B is implementing and the result is stored in the matrix C. Obviously, the number of line of the matrix B is equal to the number of column of the matrix A, otherwise the multiplication would be impossible. <fnote>See section 3.1.1. </fnote>This algorithm is straightforward and could be easily implemented. However, as we will see in the benchmarking of this algorithm, the execution time increases very quickly, exponentially. This is from a cache memory problem. <fnote>A benchmarking has been done using this algorithm on a monoprocessor machine. This benchmarking is explained in section 4.5. </fnote>The cache memory problem: In order to obtain one element of the result matrix (C), one line of the A matrix is read when one column of the B matrix is read. Hence, to obtain one line of the C matrix, only one line of A is read whereas the whole B matrix is read. The schema below (Figure 2.3.) shown that, <fnote>Schema adapted from [Bib B] </fnote><figure/>Therefore, the matrix B is entirely read to obtain one line of the C matrix. However, when a program is running on a computer, the data needed (variable owned by the program, array, program instructions etc...) are stored in the main memory (RAM) and in the cache memory. An access to the main memory is longer than an access to the cache memory; that is why the cache memory contains the last data used. Nevertheless, the cache memory capacity is not infinite and is often very smaller than the main memory. Hence, when it is full and the program, which is running, needs a data which is not available in the cache memory, then this data is inserted in the cache instead of another one. And if the program needs this other data (which has been removed from the cache) another time, this data will have to be loaded again in the cache instead of another one. That is called a cache miss. And these operations between the two kind of memory need "a lot of time". <fnote>More informations about cache miss in Bib[B] </fnote>So, if the B matrix is to large, then it won't fit in the cache. Hence, there will be a cache miss for each new row of C calculated (because B is totally read for each line of C). And a cache miss means obviously a time loss. So, depending on the cache memory size, the execution time of the sequential algorithm increases considerably after a matrix size limit. It is important to be aware of this "problem"; it helps to understand the results obtained and then why the execution time is not linear. For example, if the cache memory is 128 kilobytes. Let B a matrix of the size n containing integer number. An integer number fills 4 bytes in the memory (in general computer architecture). Hence, the matrix size can't be greater than 180 x 180, otherwise, the matrix won't fit in the cache memory and a long time will be needed because of the cache miss. <fnote>The details of the calculation are given in the appendix 8 </fnote>To solve the cache memory problem, it is possible to use a recursive algorithm which divides the matrix B in several parts to enable it to fit in the cache memory. Then, the recursive algorithm would be faster than the basic sequential one. However, this algorithm is not detailed in this report because it is not the main purpose of the project. Bib[B] gives more details concerning this recursive algorithm. Using a parallel solution to solve the matrix multiplication issue (explained in the next part) would increase the performances and reduce the cache miss (because several processors and so several cache memories will be used). <heading>The parallel algorithm:</heading>To parallelise a program, we first need to see which part of the program is parallelisable. When the way to parallelise it is found, we need to check if there are some dependencies which would make the parallelism a bit more complicated. To parallelise the matrices multiplication algorithm, several ways are available. Some famous methods have been invented such as the Cannon's algorithm. Here is explained only the algorithm which has been developed in this project. <fnote>See [Bib B] </fnote>In this part, we are going to computer the matrices product A x B = C. Hence, only these letters will be used. As we saw in the previous section, the whole matrix B has to be read to obtain one line of the matrix C, when only one line of the matrix A has to be known. To share the multiplication among several processors, we can then think to partition the matrix A and distribute each part to every processor which would be enabled to compute the product of their "allocated" part knowing the whole matrix B. The schema below (figure 2.4.) explains how this distribution is theoretically managed, <figure/>This schema shows that the matrix A is partitioned in several parts which are distributed among the processors. It is not necessary but it is better to partition the matrix in an equal number of rows. Hence, each processor would have the same number of row to compute. But that involves the number of row of the matrix has to be a multiple of the number of processor, that means, number of row mod number of processors = 0. So, each processor has a fixed number of rows from the matrix A and they also know the whole matrix B (as we saw before, this is required to compute a part of the product). Then they would be able to compute a part of the result matrix C. The range of the row computed by each processor in the matrix C is the same as the range known from the matrix A. The following representation (Figure 2.5.) shows how the computation of the product works when the matrix A is distributed, <figure/>Now, we can write the algorithm in pseudo-code which computes it. Several ways can be taken to compute this algorithm. The following (figure 2.6.) is the one which has been chosen and implemented in this project. <fnote>The algorithm is executed by EVERY process. </fnote><figure/>As we see, this algorithm is not so difficult theoretically, but some difficulties and problem can be met when it is implemented. The third part of this report explains the C implementation of this algorithm. <heading>The Jacobi method:</heading>As we explained, the Jacobi method has been implemented to solve a system of linear equations. The main goal of this implementation has been to find, using the Jacobi method, the inverse of a matrix. Actually, compute the inverse of a matrix is simply solving n systems of linear equations with n unknown variable (in fact, only the square matrix can be inversed). <fnote>See section 3.3.2 </fnote>So, in this part is explained the algorithm solving a system of linear equation and then the algorithm computing the inverse of a matrix. <heading>Sequential algorithms:</heading>First of all, the algorithm shown here (Figure 2.7.) solves simply a system of linear equations. <fnote>This algorithm is adapted from Bib[F] and Bib[B] </fnote>(This algorithm is shown on the next page.) <figure/>Using the previous algorithm, the equation AX=B can be solved, with X and B two vectors. However, we want to find the inverse of a matrix. That means we want to solve n equations containing n x n unknown variables. Indeed, solving the inverse of a matrix A of dimension n means finding the matrix A-1 such as,  FORMULA  Where In is the identity matrix of dimension n. An identity matrix can simply be defined as follows,  FORMULA  Obviously, A has to be a square matrix; otherwise it doesn't have an inverse. So, we now want to find the inverse of a matrix using the Jacobi method. That involved we want to solve the following equation,  FORMULA  Where X and B are now two bi-dimensional matrices (not a vector like in the previous algorithm) and B is equal to the identity matrix. Each column of the matrix X is actually one column of the inverse matrix and so each column of the matrix X contains n unknown variables. There are n column so there are n x n unknown variables. Hence, to find the solution, we will actually solve several single systems and gather the result of all of them. To understand how the inverse is found, here is a mathematical illustration. The first system which have to be solve to find the inverse matrix is,  FORMULA  Where Xi means the column i of the matrix X. The second system is then,  FORMULA  And so on. Hence, the algorithm solving the inverse of a matrix using the Jacobi method is simply exactly the same algorithm as the previous one but a bit adapted in order to solve several systems at the same time. The algorithm in pseudo-code follows (Figure 2.8.), <figure/>As we see, this algorithm is only an extension from the previous one. <heading>Parallel algorithms:</heading>We are going to see now the main goal of this project: parallelise an algorithm solving a system of linear equations, using here the Jacobi method. After studying mathematically the Jacobi method, we can admit that the calculation of each row of the matrix X can be done independently. Hence, the "natural way of parallelisation" can be used. This way is to separate the rows of the matrix A and to distribute them to every process. Then, each process would have a range of the matrix A and then they would solve the matrix X in the same range. The distribution of the matrix A and B is done exactly in the same condition as the parallel matrix multiplication (see Figure 2.4.). Each process would have the same number of row, which means, in this algorithm, in order to make the computation easier, the number of row n has to be a multiple of the number of processes in the system. However, the vector XPrev (see in above algorithm) which stores the last value of the X vector is not independent. Indeed, this vector has to be known entirely whatever the row. So, each processor has to know the whole vector XPrev. However, this vector changes after a k-loop. So, it has to be synchronised by each process after each execution of the k-loop. This parallel algorithm is then composed of several steps: First of all, each processor receive (or read in a file) the part of the matrix A which is assigned to it. Each process has a different part of the matrix A. Then, each process executes the Jacobi algorithm using its own part of the matrix A. So, the Jacobi method is executed ONLY in the range which has been assigned to this process. After that, each process sends to a root process, which has been designed before the execution, the matrix X which has been modified (each process has modified a range in the matrix X). When the root process has gathered all the parts of the result matrix X, it has to decide whether the convergence "is finished" or not; calculating the absolute value if each row of the matrix X - XPrev (see previous algorithm). If the algorithm is not finished (the system doesn't converge yet) then the matrix of the result found is sent to every processes. Indeed, they will need it to compute the next step of the Jacobi method. And so on until the solution is enough closed than the user expect (the system converge). The following schema (Figure 2.9.) shows, using an easier way, these different steps. In this schema, there are two sides: root process side and single process side. The root process side shows only the steps done by the root process, knowing that the root process has to do also the different steps done by a single process. So, the root process is concerned by the both sides. <figure/>It is not included directly in the report because the above schema is better to understand how it works rather than an algorithm in pseudo-code. We have seen the parallel algorithm solving the system AX=B. However we can also obtain the inverse of a matrix using the Jacobi method. The algorithm doing that would be very similar than the previous one. Indeed, as we saw, the algorithm computing the inverse of a matrix is a bit similar than the one solving a simple system AX=B, it is just an extension. Thus, we just need to modify the previous parallel algorithm to obtain the new one which computes the inverse of a matrix. So, every process would have the same number of rows from the matrix A, but would compute and solve more than one system. They will solve n systems with n equal to the size of the matrix A and the matrix B would be equal to the identity matrix with the size n. In such an algorithm, the matrix solution X would be a bi-dimensional matrix rather than a vector. <heading>Conclusion:</heading>In this part, the mathematical theory used in this project and the different algorithms solving some mathematic problems have been explained. In the project progress, this research part has been a long part and is one of the most important because it is essential to understand all these concepts before implementing them. <heading>The implemented solution in C using MPI:</heading>In this part, the implemented solution using C and MPI is detailed. Actually we won't explain again how these algorithms work (which is already done in the part 2) but we will explain how they have been translated and computed using the C language. In a first time, the main structure representing the matrix in the dynamic memory is explained, afterwards the "library" implemented in this project called "Matrix Tools" is introduced. After that the way used to implement the sequential algorithms in C, introduced in the previous part in pseudo-code, solving the mathematical issues are explained. Then the main concepts of the MPI library will be explained in order to introduce the parallel algorithms implemented. After that, the benchmarking will be analysed explaining how the matrices generators have been created and how the benchmark programs work. The results obtained on Beowulf will then be presented. Eventually, some details about the way to compile and execute these algorithms and benchmark programs on Beowulf will be given. <heading>Main matrix structure:</heading>In order to represent a matrix in the dynamic memory, a structure has been created. This structure allocates to a matrix a number of row, a number of column and a contents. This structure called MatrixType enables all the matrices to have the same form. In C, the contents is represented by an array bi-dimensional of integer. However, two kinds of matrix structures have been created: the first enables a matrix to have only integer elements and the second one enables a matrix to have float elements (useful for the Jacobi method). The schema below (Figure 3.1.) shows how this structure is implemented in C, <figure/>The contents can be of the type Integer** or Float** which simply means an array bi-dimensional, an array of array of Integer or  Float. This structure has been implemented in the file structure.h. So when a matrix is needed in a program, we just have to include this file as an header in the program file and we are then able to declare a new matrix using this following line: "MatrixType M;". <fnote>The implementation of this file is given in the appendix 7 </fnote><heading>Matrix tools file:</heading>In order to make easier some matrix handling and operations, a "library" has been created in this project. This library is contained in the files MatrixTools.c and MatrixTools.h. Thanks to this library, it is easy to manipulate a matrix structured as it is explained in the above part. Indeed this library enables the creation of a new matrix, enables to print on the screen in a format way a matrix, to read a matrix in a file etc... Here are explained only the main functions of this library, knowing that some others functions are available but not presented here. For more details about them, see the appendix 3. In this appendix 3 are also available all the signature of all these functions presented below. printStructMatrixOnScreen: Print a matrix, which is structured as explained in the previous part, on the screen in a formatted way. AllocNewMatrix: This function returns a bi-dimensional array of integers (int**). It creates an array of array of integer with a number of row/column given as a parameter. This is often used to allocate the necessary memory place for the contents of a matrix. This could be use as follows, MatrixType M; M.Contents = AllocNewMatrix(M.NbRow, M.NbColumn); MatrixCpy: This function copies the contents of a matrix in another one. readMatrixInFile: This function reads a matrix contained in a file and then creates a matrix structured (a matrix of type MatrixType). A file containing a matrix has to use a formal: the first line contains two integer numbers which are the size of the matrix and the lines following it contain the matrix (one line is one row of the matrix). Below is an example,  FORMULA  Of course, several matrices can be contained in the same file, this function has to be called several time and the matrices will be created. writeMatrixInFile: This function is does exactly the contrary as the previous one. It writes a matrix, using the syntax shown above, in a file from a matrix structured using the MatrixType. All these functions are also available when the matrix contains float elements. The name of the function is then the name + float: "functionNameFloat". MatrixProductMono: This function computes a product of two matrices using a sequential algorithm. It creates a result matrix structured using the MatrixType and returns it. It can also return only the contents of the matrix as an array of array of integers, which can be useful some times. The return type has to be indicated by a parameter. IsConvergent: This function returns whether or not a matrix is convergent. This is useful before using the Jacobi method because if the matrix is not convergent, the Jacobi method won't work. <fnote>The conditions which are to be followed by a convergent matrix are detailed in the part 2.2.2. c. </fnote>JacobiSeq: This function is the sequential algorithm in C which computes the Jacobi method. The matrix A, B and the tolerance Epsy (AX=B) are given to this function which returns a new matrix X which is the solution of the system of linear equations. JacobiSeqSolveInverse: This function computes, by a sequential way, the inverse of a matrix A given as a parameter. A convergence tolerance Epsy is also given as a parameters. It returns a new matrix which is the inverse of A (A -1). This library has to be included in every file which needs it. Actually, this library must be used to handle matrices. So, each program which computes the different algorithms in this project uses this library. <heading>The sequential programs:</heading><heading>Matrices multiplication:</heading>This algorithm is implemented in the file testMulti.c. The main function of this program is very simple. Actually, the main function just opens a file, reads the matrices in it (using the function ReadMatrixInFile from the matrix tools file), and closes the file and computes the multiplication (using the function MatrixProductMono from the matrix tools file). It get a new matrix from which is returned by the function MatrixProductMono and shows this new matrix. As we can see all the main functions used in this program are contained in the matrix tools file. We won't explain more this program here because it is easy to understand it without more explanations. We could just give some details about the creation of a new matrix (which is done in the function MatrixProductMono). <fnote>The whole code of this program is given in the appendix 4. </fnote>The dynamic creation of a new matrix is done as follows,  FORMULA  The word static means "this memory space is not only for this function, so do not delete it when the function is done". Hence, we can return this pointer and the main program can use it. For more details, see the code of this program in appendix 4. <heading>Jacobi method:</heading>This program is also composed by two file. The first one contains the main function when the second (which is the matrix tools file) contains the functions computing the Jacobi method. So, the main function (in the file called JacobiSeq.c) reads the matrix A and B in a file (AX=B), then check if this matrix is convergent (otherwise the Jacobi method cannot be used) using the function IsConvergent (in the matrix tools file), and in that case calls the function JacobiSeq (in the matrix tools file) sending by a parameter A, B and Epsy (tolerance). Then, JacobiSeq would return another matrix X which is the result of the system of linear equations. In the case where we want to compute the inverse of a matrix, it is exactly the same program but the function JacobiSeqSolveInverse (only two parameters are needed, A and Epsy) is called instead of JacobiSeq and the matrix returned would be the inverse of the matrix. We won't explain more the sequential algorithm, however the code of this function is given in the appendix 5. <heading>The parallel algorithms:</heading>The parallel algorithms have been presented in the part 2.3. We will explain here how it is possible to implement them in C using MPI on a cluster. Hence, the main MPI concepts are introduced in a first time then the specific MPI methods used in each algorithm are explained. <heading>Main MPI concepts:</heading>As we saw, MPI enables the processes in a distributed system to communicate each others. This is done via messages sent through the network. In an implemented way, MPI has to be initialized before calling any MPI method in the program. In the same idea, MPI has to be finalized at the end. <heading>MPI Initialisation/Finalisation:</heading>First of all, the library MPI has to be included in the source file,  FORMULA  This initialisation/finalisation are done as follows,  FORMULA  To understand how a parallel program works, it is important to know that every process involved in the program (if 4 processors then 4 processes) executes this initialisation. When we launch a program using the command mpirun - np 4 ./Prog (see section 2.1.3.), that means 4 processes are going to execute the program ./Prog. So, it is essential to be able to identify each of them. Hence, MyId contains the identifier of the process. This number is in the range 0..n-1 with n the number of processes. MachineName contains the name of the node (or machine) on which the process is executed and NbProc is the total number of processes. Several groups of processes may be created. However, we will use only one group which is called MPI_COMM_WORLD. This name is useful to identify all the process in our group. <heading>Concept of root process:</heading>It is sometimes helpful to define a root process. In general this root process is the number 0, but it's not compulsory. Hence, when the root process has to do something alone, which means only this process will do it, this condition has to be added,  FORMULA  For example, if the parallel application is a client/server application where the server would be the root process and the clients all the others process, then we could implement such an application as follows in the main function,  FORMULA  Client/server behaviour would be then obtained. We will use this concept of root process to implement our mathematic parallel algorithms. Indeed, this root process will have to gather the different result from all the processes etc... <heading>Matrices multiplication:</heading><heading>General:</heading>The parallel algorithm used to compute the matrix multiplication is detailed in the part 2.3.1.b. We are just going to explain how this has been done using MPI. In the algorithm the following array declaration is done, <fnote>In section 2.3.1. </fnote> FORMULA  Using MPI, this declaration is easily implemented thanks to the variable NbProc (initialized during the MPI initialisation). Then the following function has to be done,  FORMULA  It fills the matrix MyPartOfA with elements contained in A with the suitable range (depending on the process number). This function is implemented using some variables defined during the MPI initialisation such as NbProc and MyId. To understand how it works, the implementation follows,  FORMULA  The bold expression (MyId*( RowA / NbProc))+i defines the range. When each process has its own part of the matrix A, it can compute the product of MyPartOfA and B. However, at the end, we notice that the data have to be gathered. Indeed, each process has a part of the result matrix C, but the root process must to gather all these parts in one matrix C. In this program, only the root process will know the whole result matrix, so only it has declared the matrix C. All the others have only the matrix MyPartOfC. <fnote>As it is explained in the part 2.3.1.b. </fnote>In pseudo code,  FORMULA  With MPI, this operation is done using the function MPI_Gather(). <heading>MPI_Gather():</heading>This function gathers together values from a group of processes. The root process (which can be any process) receives all the parts of a variable (an array/matrix) from the processes (including it) and gathers them. The schema below (Figure 3.2.) explains this process, <figure/>The signature of this method is as follows, <fnote>More details about this function are available at Bib[C] </fnote> FORMULA  Where sendarray is the buffer containing the value sent by the process, NbElemSent is the number of elements sent, Type is the type of the sent elements, rbuf is the beginning of the buffer in which the result will be stored, root is the number of the root process (the one which will gather) and comm is the group of the communication (MPI_COMM_WORLD in our case) In our program, this is used to gather the result matrix C. Each process sends its own PartOfC and the root process 0 receives it and gathers it in another new matrix C which contains the whole result matrix. The following instruction is so used,  FORMULA  Hence the parallel multiplication is implemented in C using MPI. Actually two ways, both implementing this algorithm, have been done in this project. The first one was less optimised because it didn't use the function MPI_Gather. It could gather the value using the simple function MPI_Send and  MPI_Recv which are the default communication functions. But it is really better to use MPI_Gather to gather the value because this function has been implemented only in this purpose, so is optimised to do that. <fnote>See Bib[C] </fnote><heading>Jacobi method:</heading><heading>General:</heading>The principle of the parallel Jacobi algorithm is explained in the part 2.3.2.b. (Figure 2.9.). We are then going to explain how it has been implemented using C and MPI explaining some particular MPI methods. Unlike the matrices multiplication algorithm, each processes read the whole matrix A. Indeed, it is not necessary to separate the range for each process in another matrix My PartOfA. However, in order to compute the Jacobi method only in the range which concern it, a process has to define the Jacobi loop range as follows,  FORMULA  Hence each process knows from which until which row in the matrix A it has to compute it. The loops declaration will then be similar as follows,  FORMULA  The complex part of this algorithm follows. Indeed, as we saw, the Jacobi method calculates iteratively a solution. So, each process will have to calculate the solution of its own range for the k th iteration and then, send whether the solution is enough close from the last one (convergence sufficient or not, depending on the tolerance) or another iteration is needed. Then, each process sends the result to the root process. Hence, the root process will have to gather all these results and decide whether or not a new iteration (k+1) is needed. At the same time, each process sends the solution found for its own range to every others process. So, two kinds of data are sent: a variable which tell whether or not the convergence is enough and a matrix XPrev Range which contains the solution for the process range. Briefly, the method looks like as follows (this code has been simplified)  FORMULA  To decide whether or not each process has reached the convergence, the root process has just to check if every Stop variable is equal to 1. If it is not the case then, a new iteration is needed. Has we can see in the above algorithm, each process sends the result matrix XPrev to every others process. The function MPI_Allgather() does it. <fnote>It is sent to every others processes because they all need to know the whole matrix X. Indeed, this matrix X will become XPrevious in the next iteration (k+1). For more details, see section 2.3.2. </fnote><heading>MPI_Allgather():</heading>This function gathers data from all processes and sends it to them. So, each process sends its own part of the array and each process gathers the parts from every other process. The schema below (Figure 3.3.) explains this process, <figure/>More details concerning this function (signature, attributes etc...) are given in Bib[C]. In our application, the data type used is MPI_FLOAT, because the Jacobi method works with float numbers. <heading>MPI_Gatherv() and MPI_Allgatherv():</heading>The two last function we saw (MPI_Gather and  MPI_Allgather) work only if each process wants to gather the same number of elements. That means the number of elements has to be a multiple of the number of processes. However, the number of elements might not be a multiple of the number of processes. In this case, the function MPI_Gatherv and MPI_Allgatherv have to be used. They have exactly the same role than the previous but with a number of elements not restricted as a multiple of the number of processes. <heading>Benchmarking:</heading>Some benchmarks have been done in this project in order to evaluate the parallel solutions. In this part, we present this benchmark (how it has been done) and we analyse the result. <heading>Matrices generators:</heading>In order to be able to test the previous algorithm, some matrices samples are needed. However for very large matrices, it's impossible to create it by ourselves. So two matrices generator have been created. The first one is a simple random matrices generator and the second one is a random matrices generator which produces only convergent matrices (for the Jacobi method). <heading>Simple random matrices generator:</heading>This program creates a file which contains a matrices generated randomly. To use this program, the following syntax is used,  FORMULA  This command creates the file FileName containing a matrix with 400 rows and 400 columns. The last parameter (new or  save) determines whether the matrix is added in the file or the file is emptied before writing the matrix. So, if we want to write two matrices in the same file, then the parameters save has to be used. <heading>Random convergent matrices generator:</heading>This program generates also a random matrix but only a convergent matrix. To be sure that the matrix generated randomly will be convergent (so be able to used with the Jacobi method), on each row generated, the diagonal element (aii) will be always greater than the sum of the others elements in the same row. <fnote>The rules determining whether or not a matrix is convergent are defined in the section 2.2.2.c </fnote>To use this program, the syntax is as follows,  FORMULA  And the parameters are the same as the previous generator. <heading>Matrix multiplication benchmarks:</heading><heading> The benchmarks programs:</heading>A program which performs a lot of tests, using the program explained above, has been created in order to compare the sequential algorithm performance and the parallel algorithm performance. Here is explained how this benchmark program works and one execution will be presented and the results detailed. Two different benchmarks programs work: one for the sequential algorithm and the other for the parallel algorithm. Both execute the mathematical program n times, every time with a different matrix size and the execution time is tracked in a file. When the executions are finished, we just have to copy the result file (which contains all the execution time) and create some graph to analyse the performances. To execute one of both benchmarks programs, the following command is used,  FORMULA  This command would execute the benchmark program which will execute the multiplication matrices algorithm: from 10 to 1000 step 10 means: the benchmark will start with two matrices of size 10 x 10 (multiplication), then will execute another test with a matrix 20 x 20 (because 10 + step = 10 + 10), and so one until 1000 x 1000. So, this command would execute exactly 99 tests, tracking each time the execution time. Of course, each time the benchmark program run a next test, it will create two new matrices using the matrix generator before computing the product of these matrices, and of course, the execution time of the matrix generator is not tracked because it's not the purpose of the project. Only the mathematical algorithms are timed. These benchmarks programs use the system function fork() to execute one test and to execute the matrix generator. Some shell scripts have been created to ease the benchmark execution, but they are not detailed here. They would be explained during the demonstration. <heading>The results:</heading>Some different tests have been executed. Here is presented the result of one of them. This benchmark has been executed once using the sequential algorithm and another time using the parallel way. Hence, we will be able to calculate the speedup and efficiency of this parallel algorithm. This benchmark executed the algorithm with matrices size from 12 to 972 with a step of 60. So, 17 tests have been done. The sequential program has been executed one 4 processors on Beowulf. The execution results are in the appendix 6. We will analyse the result drawing some representative of the performance graphs. On the first graph (Figure 3.4.) are two curves: the first one is the execution time depending on the matrix size using the sequential algorithm and the second one is the execution time using the parallel algorithm on 4 processors. <figure/>As we can see on this graph, the parallel algorithm is faster than the sequential one, especially when the matrices are very large. Indeed, the parallel algorithm becomes very quicker when the matrices are larger than 400 x 400. However, we can notice that for matrices smaller than 400 x 400, the parallel algorithm is less efficiency than the sequential one. This is because some factors (network, communication inter processes, processes synchronisation, etc...) are added in the parallel way. So, to be efficiency, the parallel algorithm needs large matrices. We can also notice that the multiprocessors curve is less linear than the other one (there is some waves). This can be explained because the network (between the nodes) is not always in the same state. That means, at one time it can be more overloaded than another time. Is it very gainful to use four processors instead of only one for such an application? To answer, we can draw the speedup graph (Figure 3.5.) and the efficient graph (Figure 3.6.), <figure/><figure/>We can see on these graphs that the results are not bad at all. The maximum speedup is 2.6 for an efficiency of nearly 65%. Furthermore, the general aspect of these two curves is good because they grow up considerably. <heading>Jacobi method benchmark:</heading>The Jacobi method works using a sequential way as well as a parallel way. Some tests have been done, with several matrices sizes, using either a simple or multiple (to compute the inverse of a matrix A) system of linear equations. However, such benchmarks (as the matrices multiplication) have unfortunately not been done using the Jacobi method because some difficulties and some problems concerning MPI on Beowulf came and took some times to be solved. So, no graph can be drawn but we can analyse the results found. The result matrix found using the sequential way and the parallel way are exactly the same which means both works well (the result have been checked). However, the execution time using the parallel way was always longer than the one using the sequential way; at least with for all the tests which have been done (the matrices were not larger than 500 x 500). The parallel algorithm has been tested using 4, then 6 processors. We could explain these results because as we saw in the algorithm, a data gather is often done. After discussing about these unsatisfactory results with my supervisor, Chris Cox, I have understood that the Jacobi method is efficient with very large matrices (larger than the one I used) and with very specifics matrices (matrices which are dense around the diagonal). After this discussion, I checked again if I could obtain better result using the parallel algorithm, but I didn't manage to obtain them. I have also checked again whether the matrix result obtained with the parallel algorithm was correct in order to be sure there was no mistake in the code and I realized that the matrix result was correct. So, the algorithm works but to be efficient using several processors the matrices representing the system of linear equations has to be very large. <heading>Conclusion:</heading>We have seen in this project several technologies and concepts concerning parallel mathematical applications, distributed computing etc... A lot of these technologies are still developed and are the subject of a lot of research projects throughout the world. As we saw, distributed computing is a very large and interesting subject, but complex. That is why this project involved a lot of researches dealing with these technologies and concepts which I nearly didn't know before. The most difficult part has been the understanding part. Indeed, I had to understand the concepts dealing with the computing part (distributed computing, parallel algorithm, Message Passing Interface, etc...) and the mathematical part (different properties of matrices, Jacobi method, etc...). But after this part, came the practical part which involved the development of my own mathematical applications. This part was also very interesting because I could try concretely all the concepts I had read before. Work using a cluster such as the Brookes Beowulf was totally new for. It was a real great opportunity to be able to use it because we can't use such a cluster "every day" at home. Applications which have been created in this project gave some interesting results, such as the results obtained by the matrices multiplication program with a speedup not bad at all and promising. The results obtained, with the Jacobi method application, were good because we could obtain the inverse of a matrix very quickly and using either a sequential or a parallel way. However, the performances obtained with the Jacobi method application were not good in a first time because the parallel way was longer than the sequential one. But after a discussion with my supervisor, I have understood that the Jacobi parallel method needs very large matrices to be efficient. To conclude, this project was very interesting and I really learnt a lot about new concepts (for me), thanks to it. 