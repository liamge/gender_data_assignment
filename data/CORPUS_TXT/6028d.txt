<heading>INTRODUCTION</heading>Foreign language teaching is a broad field associated with many issues one of which being testing. Tests offer an insight to learners' progress and give teachers the chance to assess their own success in the transfer of knowledge. So many researchers have worked on this issue by developing different approaches to testing as well as different types of tests. As testing has a crucial effect on teaching, teachers need to be sure that tests learners take are valid. Thus, every test should be related to a specific context as students' needs and characteristics differ from one country to the other and even more specifically from one class to the other. Therefore, this paper aims to evaluate the reading part of FCE the "most widely taken Cambridge EFL examination" (UCLES, 2001: 6) in relation to a specific teaching situation. The evaluation following Weir's framework for reading addresses the theory based, context and scoring validity. The analysis starts by setting the test to its context and then provides a brief literature review on reading and its testing before it proceeds to the discussion of the test's validity in relation to the three aspects mentioned above. <heading>LITERATURE REVIEW</heading>This section provides a brief overview of the basic concepts of language testing. At the outset, a variety of different types of tests has been developed over the past decades. The basic kinds are four: achievement, proficiency, diagnostic and placement tests. Achievement tests evaluate learners' progress throughout a course (0' Sullivan, 2005: 2) and are based on the material covered in it. On the contrary, proficiency tests are not related to a specific course but assess students' general language ability. Diagnostic tests "diagnose students' strengths and weaknesses" (Hughes, 1989: 7) whereas placement tests provide information about placing a learner to the appropriate class within a language teaching program (Johnson, 2001: 293). Tests vary also in the format they follow, as they can measure directly or indirectly learners' ability to perform a skill. Additionally, tasks involved can be "discrete points (where each task involved the use of just one language element) or integrative" (O' Sullivan, 2005: 9), whereas the scores can be marked objectively or subjectively. Additionally, results can be reported in different ways depending on the test's purpose. Particularly, assessing reading is a rather difficult task as no one can really define what takes place in learners' mind when reading a text. However, this paper evaluates the reading part of FCE exams since it is very challenging and interesting to see how people manage to construct a test of a traditionally considered 'receptive' skill. Yet before referring to it, attention will be paid to the teaching of reading because as Alderson (2001: 31) states: "an understanding of the nature of reading is crucial to the development of our assessment instruments". So, during 1960's where Audiolingualism was prominent, reading was only learned to assist spoken instruction (Grabe, 1991: 376). A decade later, reading involved an active understanding of a wide range of texts that demanded the activation of various learning strategies (Grabe, 1991: 377). Today, reading is not seen as a receptive skill but as one that engages the reader in an interaction with the text, enabling him to draw inferences by activating his "relevant schemata" (Ur, 1996: 108). Additionally, this variety of reading texts should aim to "engage [readers] in purposeful reading" (Hedge, 2000: 221). Similarly, work on the testing of reading until the early 1980's focused on the formats used to test it (Urquhart & Weir, 1998: 150) and "provided no information on learners' ability to use language for communicative purposes" (Brindley, 2001: 139). However, current research focuses on direct tests which assess readers' ability to "understand texts in a range of different topics" (Alderson, 2000: 63). <heading>CONTEXT</heading>As every test should be immediately linked to a teaching context so FCE reading test addresses students at the intermediate level. Particularly, in Greek society where people start learning English from the age of 9, FCE examinations constitute the first certificate that Greek students hold. English is being learned mainly for general academic knowledge, for job qualification, for traveling and for communicating with speakers of other languages. In this possible context which I will be teaching in the future, English is taught 6 hours a week at a private language school with the help of a textbook, past papers of FCE and blackboard. The class consists of 10 Greek mixed ability learners and the main aim is to teach them how to communicate adequately in all 4 skills outside class. More specific objectives concern the teaching of language items, functions and skills according to their level. <heading>VALIDITY: AN IMPORTANT ELEMENT</heading>All the research done in testing highlights its importance and immediate need in language teaching. Tests are taken to assess learners' language abilities (Bachman, 1990: 2), identify their strong and weak points and evaluate their overall progress (Bachman, 1990: 3). So teachers can reflect on their teaching and make any changes if required. However, as teachers need to base their lesson on a plan, so test developers need to construct their tests following certain criteria with the most important being validity, the degree to which a test measures what it claims to measure (Popham, 1991: 55) or as the American Psychological Association (1985) supports: "the extent to which inferences we make on the basis of test scores are meaningful, appropriate and useful" (cited in Bachman, 1990: 25). Many researchers have attempted to evaluate tests by constructing various frameworks since these depict closely what we aim to test (Weir, 1993: 20). Therefore, this paper provides evidence for the three aspects of validity covered by the FCE exam: theory-based, context and scoring validity, following the complete and up-to-date Weir's (2005) framework. The discussion starts by the presentation of the tables of the three validity aspects and continues with a detailed analysis of them. <heading>DISCUSSION</heading><table/><heading>Test takers' characteristics</heading>To begin with, all tests should be designed by keeping learners and their specific traits in mind. So the test taker table is linked to the theory-based validity one because these characteristics affect the way candidates process the test tasks (Weir, 2005: 51). Test taker characteristics fall into three categories: physical, psychological and experiential. The physical characteristics concern short-term ailments such as toothache, or cold, longer-term disabilities in speaking, hearing or vision, the age and candidates' sex. On the other hand, the topic of reading text should be interesting to learners and appropriate to their age and sex. Additionally, tests should allow for the provision of accommodation for students with disabilities as FCE does. If candidates have any longer-term disabilities they can ask for special arrangements to be made (UCLES, 2001: 8). Particularly, students with visual difficulties can be given extra time to complete a paper or read the questions in a different way or write their answers using the Braille version, or be given the questions in enlarged papers. Finally, they can have a reader reading the questions out to the candidate (Cambridge ESOL, 08/04/2006). Special consideration is also offered to candidates "affected by adverse circumstances before or during an examination" (UCLES, 2001: 8). Furthermore, the reading part of FCE exams caters for learners' psychological characteristics by including texts from a wide range of sources (UCLES, 2001: 10). Texts referring to activity holidays, to shopping or to someone's experience of learning to fly "motivate a deeper reading since they are linked to students' academic knowledge and leisure interests" (Alderson, 2000: 29). So students' schemata are activated and their varied language abilities are tested. Topics (...) which might introduce a bias against any group of candidates (...) are also avoided" (UCLES, 2001: 6). Finally, the test considering learners' experiential traits ensures candidates' familiarity with the exam format and the task types by the availability of coursebooks, practice materials, and past examination papers (UCLES, 2001: 8). Moreover, the specification of the testers' expectation leaves students with adequate time to prepare themselves for taking the test. So the test challenges learners' reading abilities while catering for any special as well as general needs. <heading>Theory-based Validity</heading>This validity type refers to whether the cognitive processes needed to carry out the task are appropriate or not (O' Sullivan, 2005: 16). It is divided into executive processes (goal setting, visual recognition, pattern synthesizing) and resources (language and content knowledge). The first element of executive processes provides candidates with a clear idea of the reading purpose so that they can employ the appropriate reading strategies (UCLES, 2001: 10). Candidates "are expected to (...) read semi-authentic texts of various kinds (...) and to show understanding of gist, detail, and text structure and deduce meaning" (UCLES, 2001: 7). The goals of the reading passages are also determined by the instructions provided in the rubric prior to each test. The first activity for instance asks learners to choose the most appropriate heading from a list (Appendix). Visual recognition, the second element of theory - based validity, is ensured by the actual presentation of the test. The input is clearly printed and well-presented. The various parts are easily recognized as they are written in bold letters whereas overload of information is avoided so that the material can be legible. Moving on to the executive resources available to candidates, it should be noted that grammatical, textual, functional and sociolinguistic knowledge is demanded from them. Students' textual knowledge is examined in part three of the test which "tests understanding of how tests are structured and the ability to follow text development" (UCLES, 2001: 10). The test also assesses students' understanding of the texts' function and interpretation of their meaning. The second reading passage examines learners' functional knowledge by asking them to answer questions regarding the opinions expressed in the text. So candidates can evaluate whether certain utterances are intended "to convey ideas, manipulate, (...) as well as understand indirect speech acts and pragmatic implications" as Buck (2001: 104 quoted by Weir, 2005: 97) claims. Students' grammatical knowledge is also indirectly assessed since their comprehension of passages and completion of test presupposes knowledge of the according syntactic and grammatical items. Lastly, test developers take into account candidates' sociolinguistic knowledge. For example, the second passage where a father describes his relationship with his son (Appendix) activates students' knowledge of appropriate linguistic forms such as the use of the first person singular and informal language. Though there is no clear inference to any particular sociolinguistic group, the instructions really help candidates to use any sociolinguistic knowledge that might be applicable. Furthermore, candidates' internal knowledge is evidenced in the Information Sheet that all of them complete by defining their age, nationality and educational background. (UCLES, 2001: 7). So language testers can estimate more or less some of the knowledge students bring to the test. External knowledge on the other hand, is provided in the test's rubric and the passages (Appendix). Students get information about the content of the test from the instructions, the title as well as the actual reading of it. For example, instructions in part 2 (Appendix) provide students with knowledge about the text. Additionally, by reading the passage students learn more about the relationship of the two men. At the same time, this external knowledge activates learners' own schemata and background topical knowledge. <heading>CONTEXT VALIDITY</heading>The second major category of a test's validity considers the task's settings and linguistic demands as well as the issues related to the test's administration. The test is divided into four different parts and the settings of each task are described in the Handbook (UCLES, 2001). Specifically, the main purpose of part one is to assess learners' ability to search for main ideas (UCLES, 2001: 10). Similarly, part 2 tests "detailed understanding of a text" (UCLES, 2001: 10) whereas part 3 assesses comprehension of the test's structure (UCLES, 2001: 10) where students choose from a list of paragraphs which one fits each gap. Finally, part 4 requires from students to read for specific information (UCLES, 2001: 11) and sets what they have to do in the instructions included before the passage. However, if the instructions stated clearer the strategies being tested, students' motivation would increase since the reading would be more purposeful (Weir, 1993: 65). Additionally, the assessment criteria are not applicable since none of the tasks requires detailed responses by the candidates. In contrast, all of them are different versions of multiple choice or matching items. The test also ensures that the weighting of each item is known to candidates as "questions in parts 1, 2 and 3 carry two marks [whereas] (...) questions in part 4 carry one mark" (UCLES, 2001: 9). So students knowing that each part is weighted approximately equally (UCLES, 2001: 53) they can allocate the 75 minutes given to them accordingly. Besides, test developers try to ensure that there is enough time allowed for all tasks by trialing the test before administering it. Specifically, as it is stated by UCLES (2001: 6) "after selection and editing, the items are compiled into pre-test papers". In this way it is ensured that the test conforms to its specifications. So candidates need to be very concentrated to answer the questions within the time allowed. However, the time constraint that is set does not reflect the time people are given to read something in real life which is usually without constraints. Of course this would be reasonably inapplicable in a testing situation. Moving on to the discussion of the texts' length, it is obvious that the first passage is the shortest one whereas the third is the longest. Texts 2 and 4 on the other hand, have more or less the same length. As mentioned in the Handbook (UCLES, 2001: 9) each text has 350-700 words approximately and the whole test 1900-2300 words. Furthermore, the test format "should specify test structure" (Hughes, 1989: 50) which is in fact done in the handbook (UCLES, 2001: 9). Apart from the third passage where students have to place the paragraphs of a jumbled text in order, all the other passages require multiple matching or multiple choice answers. Even though these techniques are objective yet the difficulty with the MCQ formats is that "the tester does not know why the candidate responded the way she did" (Alderson, 2000: 212) so they allow for the possibility of guessing. Finally the order of items should be carefully considered by the test. This test particularly separates into distinct parts the items focusing on different reading strategies. So it is evident from the test that scanning and skimming are tested in different texts so that learners do not get confused with the reading purpose and the task's expectation. Apart from the consideration of the task's characteristics, test developers specify the way it is administered. Particularly, they have created a team responsible for the administration of the test because "if the test is not well administered, unreliable results may occur" (Weir, 2005: 82). So based on studies conducted by Cambridge ESOL (08/04/2006) examination centres are independent institutions which administer the examinations by following strict and detailed procedures. These centres are regularly inspected and exam invigilators are provided with all the administrative details concerning the existence of non-distressing physical conditions and the uniformity of administration so that testing conditions are the same for every testing centre. Security is also given a high priority. Cambridge ESOL (08/04/2006) also ensures that the examination dates are carefully selected and safety measures are taken at each stage of the examination process. <table/>As far as the linguistic demands of the task are concerned, the test ensures that all the resources in the text and those required by the candidates are a bit challenging but appropriate for their level. The vocabulary used, taken from traveling, flying and shopping is part of the "language forms candidates (...) [are] expected to handle" (Weir, 2005: 78). The syntactic structures used in the text are according to the grammar students were taught in the FCE published coursebooks. The functions students should know at this level are also stated in the Handbook. So, the test by assessing learners' skimming and scanning abilities, examines what it claims to examine. Therefore it is up to students to practice adequately these strategies so that they do not meet any serious problems when taking the test. The discourse mode should also be appropriate for the skills and strategies being tested. Students should be able to read semi-authentic texts from magazine articles that they will be reading in their future target situations. Passages 3 and 4 are anonymous while the writer's name in the first two passages is known to learners. However, the latter might allow for the possibility of learners' familiarity with the writer which though would constitute an important part of their background knowledge, it might also increase bias against those candidates who are not familiar with the writer. So the test, in an attempt to keep a balance between the two, includes both anonymous and signed texts, taking into account the writer-reader relationship. Finally, the content knowledge required by students is appropriate for their level since the topics selected have wide appeal to heterogeneous group of students who are familiar to them and can activate their schemata. <heading>SCORING VALIDITY</heading>The last type of validity to be evaluated concerns the way grading and results are given and is the scoring validity. The first element, item analysis follows certain procedures for the "investigating properties of test items (...) prior to [the] development of their final format and content" (Weir, 2005: 202). So FCE measures items' difficulty during the pre-testing phase. Test developers send the test to various centers and after these are completed by students they are returned to the "Pretesting Section of UCLES EFL [where] (...) the items are marked on and analyzed and those which are found to be suitable are banked" (UCLES, 2001: 6). Apart from it, the test's internal consistency is evaluated and measured. Internal consistency is "the extent to which individual items are functioning in about the same way" (Popham, 1991: 55). Particularly, Cambridge ESOL estimates it using a formula called "Cronbach's alpha" (Weir, 2005: 30). An example of such an analysis using this formula is depicted by Geranpayeh (2004: 21-22). <table/>Error of measurement on the other hand, is concerned with "the difference between an observed score and the corresponding true score, or proficiency" (Weir, 2005: 204). Tiredness, lack of motivation and poor health are some of the obvious sources of errors of measurement since they influence candidates' performance (Bachman, 1990: 160). So FCE exam developers estimate the standard error of measurement based on a system that Luoma (2004) describes (cited in Weir, 2005: 33). Lastly, marker reliability ensures that tests are marked by all examiners consistently with the application of the same criteria. Particularly, "objective tests (as the one evaluated) are marked by a system of on-site supervised marking, and there is extensive use of computerised methods of marking, involving optical mark readers" (ALTE 2006: 2). As Weir (2005: 201) claims: "in receptive tests not much concern is on marker reliability per se". So this objective way of marking the test ensures that grading will be consistent and fair. Besides, one of the advantages of MCQ tests is that they "exhibit almost complete marker reliability" (Urquhart & Weir, 1998: 158). <heading>8. CONCLUSION</heading>In conclusion, this paper evaluated the FCE reading test with regards to Theory-based, Context and Scoring Validity. Overall, evidence for the test's validity was easily found from the Cambridge website, the Handbook and the test itself. However it was difficult to find information for some aspects of validity, especially for the scoring part and the executive resources included in theory-based validity as there was not much research done regarding reading. Nevertheless, the whole attempt of evaluating a test was really challenging, motivating, and helpful for inexperienced teachers like me. The test was a good one since it engaged students in purposeful reading of a wide range of interesting to them topics and had all the characteristics that Alderson (2000: 83) sees in good tests of reading. So I would definitely recommend the test to my students since it is taken by many other Greek candidates over the past years and it is complete, adequately trialed, as well as in accordance with my learners' needs. Actually, UCLES has been working a lot on developing valid tests so all teachers should learn how to evaluate certain tests according to their specific teaching context. This would offer a "clearer understanding of what language proficiency is and the order in which our students acquire it" (Weir, 1993: 170). 