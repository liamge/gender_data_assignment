<list></list><abstract><heading>Summary</heading>When designing a communications system or defining a communications standard, a model would be used to see test how different variations would perform in certain scenarios. This project built up such a model to compare a range of modulation schemes and error protection methods. The knowledge gained from this process enabled the full analysis of a complete 802.11a model at the end of the project. The main performance gauge used in this project was a systems ability to cope with channel noise. 3 types of channel noise were studied, additive white Gaussian noise, flat fading and dispersive fading. BPSK, QPSK, 16-PSK, 16-QAM and 64-QAM modulation schemes were compared, using bit error rate curves and packet error rate curves. Bit error rates were calculated by comparing the demodulated message with the original source message. Packet error rates were calculated at the receiver using a 16-bit CRC added to the end of each packet. The system model also includes convolutional coding, a type of forward error correction. Two types of codes were analysed. A simple 3,[7,5] code was used to explain the principles of convolutional coding, but the main focus of the project was on 7,[171,133] NASA coding, which is very common. Messages were decoded using a Viterbi decoder. The 802.11a model, supplied as a demo with MatLab, brings together all the system components discussed in this project, along with a multi-carrier multiplexing technique known as orthogonal frequency division multiplexing (OFDM). The model was implemented in Simulink. Explanations and justifications for each component of the model are given. The conclusions section of this report covers common issues and decisions that are faced when designing a communications system, and some recommendations for further modelling work are given at the end. </abstract><heading>Introduction</heading>As the majority of communications head into digital territory, digital signal processing is playing an ever larger role. Radio frequencies are becoming more crowded and services are requiring greater data rates, resulting in the realization of more complex modulation and coding schemes. Several stages of modulation and coding are often combined to maximise performance, and all this must be done inside a digital signal processor. During the development of a communications system the manufacturer hopes to achieve: <list>resilience to noise and signal echoesfast data ratesreliable data deliverylower costs</list>Clearly the above requirements conflict, and there must be a trade-off. A manufacturer will use a system model to examine what modulation and coding schemes are available to give the best balance of the requirements. This project will build a complete model of the physical layer of a communications system, from a serial binary source to a serial binary output at a receiver, and analyse the complete 802.11a wireless LAN model. <heading>Design Philosophy</heading><heading>Noise</heading>Every signal which is transmitted through the atmosphere is subjected to atmospheric noise, the single most detrimental factor in most communications systems. However, there is also noise generated inside the electronic equipment itself, such as shot noise (which is caused by the movements of discrete charge carriers instead of an ideal smooth current). The various types of noise all roughly follow a Gaussian distribution, which means they can be combined and included as one element in the system model. In practice, the noise power would stay roughly constant, and the signal power would vary depending upon quality of reception. However, it is simpler to model by maintaining constant signal power and varying the level of Gaussian white noise added to it, the approach used in this project. Fortunately digital signals are inherently more resistant to noise than their analog counterparts, and the rest of this report will demonstrate the methods used to counteract the problem of noise. <heading>Digital modulation</heading>For several reasons, in order to transmit a digital signal over an analog wave guide, such as a radio wave, it must be modulated: A binary signal will contain a DC offset, which should be removed before transmission. Without modulation the natural analog representation of a binary zero appears no different to if there were no signal received at all. The bandwidth of a digital waveform is theoretically infinite, and must be reduced to make its transmission practical. Transmissions must be modulated to the frequency band which has been licensed by the appropriate governing body. The system model was designed to use five common types of digital modulation: <list>BPSK (Bi-Phase Shift Keying)QPSK (Quadrature Phase Shift Keying )16-PSK16-QAM (Quadrature Amplitude Modulation )64-QAM</list>All of these modulation schemes have been implemented in many communications standards, such as the 802.11a wireless networking standard. They all involve converting the binary signal into complex symbols (i.e. numbers with a real and imaginary part). Complex numbers can be represented by a magnitude and a phase, which makes them perfect for modulating an analog carrier frequency. There are other modulation schemes that do not work in this way, but they will not be discussed here as they are far less common. <heading>BPSK (1 bit per symbol)</heading>The binary data is modulated to a carrier at constant amplitude. The phase will be ordinary to represent a binary zero, or inverted 180 degrees for a binary 1. <heading>QPSK (2 bits per symbol)</heading>The binary values are taken in pairs, so that each pair can represent 1 of 4 combinations. The 4 symbols have the same amplitude, but are phase shifted to be 90 degrees apart from each other. This is the meaning of quadrature. The complex values are chosen so that they all have a real component, as shown on the argand diagram Figure 2.1. Otherwise, a complex symbol with no real part would in practice have zero power, and could not be transmitted. Since the binary values are modulated in pairs, the baud rate of this scheme is double that of BPSK. To compensate, the resilience to noise is lower. <figure/><heading>16-PSK (4 bits per symbol)</heading>The expansion of BPSK to QPSK is taken a step further. The binary bits are grouped into sets of 4, each representing 1 of 16 combinations. This modulation scheme is mathematically sound, but the phase difference between adjacent symbols is fairly small, as can be seen in Figure 2.2. This can yield a bad SNR at the receiver, and so this scheme would often be replaced by 16-QAM. <figure/><heading>16-QAM (4 bits per symbol)</heading>Increasing the number of bits per symbol any further with a PSK system would result in a modulation scheme that is not much use in any type of noisy environment. Instead the power level of each symbol is adjusted to distinguish more clearly between adjacent symbols, see Figure 2.3. When setting a power level to use for a QAM scheme, the average power is used. 16-QAM performs favourably compared with 16-PSK. <figure/><heading>64-QAM (6 bits per symbol)</heading>In optimal noise conditions this scheme can provide a data rate 6 times faster than BPSK. <heading>Convolutional coding</heading>Convolutional coding is one of many coding methods that add redundancy to a signal in order to make its transmission more reliable. <heading>Encoding</heading>The encoding process is often implemented as, and best visualised as, a shift register with taps taken off at some of the flip-flops. The bits from these taps are modulo-2 added to produce two encoded bit streams, grouped together as one symbol. The system model was tested with two convolutional codes:  FORMULA   FORMULA  The latter is known as NASA convolutional code. The former is an arbitrary, simple code designed for this project. In each notation, the first number is the number of flip-flops that are in the shift register. The numbers in square brackets are octal representations of where the taps are placed along the shift register in order to produce the two output symbols. Taking the example 3, [7,5] gives the encoder shown in Figure 2.4. Note the octal number 7 is 111b in binary, and each tap is used to create the first symbol, C 0. The octal number 5 is 101b, thus only the first and last taps are used to create the second symbol, C 1. <figure/>Both of the convolutional codes mentioned in the previous paragraph are called half-rate coding schemes. This describes the way that the encoder will take 1 input bit and create 2 output bits, as in Figure 2.4. There are other fraction-rate schemes, but half-rate is the most common. <heading>Decoding</heading>The Viterbi algorithm is most commonly used to decode signals that have undergone convolutional coding. The algorithm is complicated, but an outline of the process should suffice for comprehension of this report: The decoder waits for a predetermined number of symbols. Starting with the last received symbol, it employs a statistic process of estimating the previous symbols, known as a traceback. The process can be visualised most easily as a trellis, and is sometimes called a trellis decoder. Symbol errors can be overcome, because the decoder will decide that a different symbol was more likely. The Viterbi decoder will be the same for any convolutional code, it just needs the parameters of the code in use. The decoding process introduces a delay because the traceback outputs symbols in reverse order. <heading>Puncturing</heading>Puncturing is a technique used to increase data rate at the expense of noise immunity, by removing a regular pattern of symbols just after convolutional coding is carried out. It is a method of fine tuning the data rate, to obtain maximum throughput from a given coding and modulation scheme, which might otherwise exhibit an unnecessary level of redundancy. Recalling that coding schemes can be desribed by a rate. The convolutional codes in this project are half-rate. Puncturing will alter the coding rate of the scheme as shown in equation 1.  FORMULA  (1) For example, a 2/3 rate coding scheme (i.e. 2 uncoded bits in, 3 coded bits out) could be created using a half-rate encoder (1 bit in, 2 bits out) and a 4/3 puncturing rate (4 bits in, 3 bits out) as shown in equation 2.  FORMULA  (2) <heading>Packetisation</heading>Like the majority of digital communications systems, the model produced in this project groups bits into packets. There are a several of reasons to do this: The last few bits in each packet can be set to zero, so that the Viterbi traceback can be guaranteed to start at the right value. Now the Viterbi decoding process can be reset at the start of each packet, which has an enormous improvement on the delay that is introduced and only uses a fraction of the processing power. Setting the end of each packet to zero is known as state pinning. Extra stages of error protection can be added to packets (often called outer coding). These techniques are used to protect against different types of errors than the convolutional coding discussed previously. Typically, outer coding will be used along with interleaving to protect against a sudden burst of noise. A cyclic redundancy check can be added to each packet, so the receiver can determine the quality of data it is obtaining. Packets of 128 bits were used in this model, with 120 data bits and 8 padding zeroes. <heading>Cyclic Redundancy Check (CRC)</heading>A cyclic redundancy check involves adding more redundancy to a data signal. This redundancy cannot be used to correct errors, but is used to tell the receiver whether a packet has been received with guaranteed 100% accuracy. A CRC is often used for systems in which retransmission of data can be requested. If retransmission cannot be requested, such as with a broadcast, then the data must be accepted regardless of its quality and so the result of a CRC would be of little consequence. However, it is not always so simple, and a CRC is included in DVB broadcasts, which have several layers of error protection, some of which are too complex to discuss in this report. A CRC is included in the 802.11a standard, but it is not carried out in the physical layer, therefore it does not appear in the 802.11a model studied in this project. However, the single carrier modulation model does include a CRC, whereby the result is added at the end of a data packet. This model is perfectly adequate to demonstrate the use of a CRC. When the CRC was introduced into the model, the amount of data in a packet was reduced to 104 bits, to allow for a 16-bit CRC to be added to the end of each packet, bearing in mind that 8 bits are zeros for state-pinning. Only 104 bits of data are now being sent per packet, with an 18.75% overhead, thus a CRC should only be included in a system if it is necessary. In the majority of cases in this project, the quality of the system has been tested using the bit error rate (BER - i.e. how many bits are in error at the receiver). This requires a direct comparison between the source and destination, which is clearly not possible in most systems. A CRC can be used to calculate the packet error rate (PER), thus the inclusion of a CRC allows the receiver to determine the quality of its data without having access to the original message. <heading>Multi-carrier modulation</heading>Communications standards have several methods of increasing throughput over a given channel bandwidth. Older methods like TDMA (time division multiple access) and now CDMA (code division multiple access) are being replaced by OFDM, particularly in Europe. OFDM is Orthogonal Frequency Division Multiplexing. It is a multiplexing technique, rather than a modulation technique, since a signal will have already been modulated using one of the single carrier methods before OFDM is carried out on it. However, when the system is considered as a whole, OFDM provides a method of multi-carrier modulation where a number of symbols can be transmitted at the same time, each on a different frequency carrier wave. This serial to parallel type conversion means that each symbol is allowed to exist in the signal for a much greater length of time, giving it resilience to echoes that might be caused by multipath reception of a signal. The 802.11a wireless LAN standard is an ideal multi-carrier system to analyse. <heading>Overview</heading>Figure 2.5 shows the general overview of the system model. <figure/><heading>Implementation</heading>This section of the report details the implementation of the system model. Simulink was the obvious choice of implementation tool, since many of the complex coding tasks are provided in ready-to-use blocks. The model was built up in stages so that each part could be checked separately. <heading>Single carrier modulation schemes</heading>Figure 3.1 shows the first stage of the model. Before continuing to implement features of the communications system, this part of the model was used to compare the 5 single carrier modulation schemes used in the project, described in section 2.2. <figure/><heading>Bernoulli Binary Generator</heading>This block is a pseudo-random binary generator. It can be run for any required length of time thus it is ideal for as the signal source for the model. The generator requests a parameter called 'samples per frame'. While analysing modulation alone, this parameter was set the same as the number of bits per symbol of the modulation scheme in use. <heading>To workspace</heading>There are blocks included that take an stream of data and record it into an array, one is labelled 'source', the other 'demod'. These are useful for manually inspecting the data in the MatLab command window, where data can be used for other purposes, or to see how the demodulated output differs from the input signal. <heading>Bit to integer converter</heading>This block forms the first part of the modulation process. The binary bits at the input are grouped into symbols (the number of bits per symbol varies with the modulation scheme used at the time). At this point, the symbols are in the form of positive integers. The 'integer to bit converter' performs the opposite transformation further along the system chain, and outputs demodulated binary bits. <heading>M-PSK modulator</heading>This is the second part of the modulation process. Inside the block parameters, variable 'M' is set according the desired modulation scheme. 'M' will be equal to the square of the number of bits per symbol in the 'bit to integer converter' block, since there are n-squared possible combinations of an n-bit number. This block accepts the positive integers from the previous block, and turns them into complex numbers with the correct magnitude and phase, as discussed in section 2.2. The 'M-PSK demodulator' reverses the modulation and sends positive integers to the integer to bit converter. <heading>Rectangular QAM Modulator</heading>If the desired modulation scheme was a variant of QAM rather than of PSK (i.e. 16-QAM and 64-QAM), then the 'M-PSK modulator' was replaced by a 'Rectangular QAM Modulator' as shown in Figure 3.2 (and the corresponding demodulator was replaced by 'Rectangular QAM Deodulator'). This block has a few more parameters that can be adjusted. 'M' was set in the same way as for PSK schemes. Average power was used for normalization, as is the case in the international 802.11a standard. <heading>AWGN channel</heading>AWGN is 'Additive White Gaussian Noise'. The input signal is passed through with Gaussian noise added to it to create a user-specified SNR. <heading>Discrete time scatter plot scope</heading>A visual output is given, in the form of an Argand diagram. The plot can be told to hold a specified number of previous plot points, so that a scatter plot can be created. The plots given on this scope can be used to verify that the modulation process is performing as explained in section 2.2. However, the scope output will vary slightly to the argand diagrams shown in Figure 2.1-2.5 as it will exhibit a certain amount of noise depending on the SNR specified in the AWGN block. <heading>Error rate calculation</heading>The output of the modulation/demodulation process is compared directly to the input signal from the binary generator. A display block is used to show the results of this comparison, which are: <list>total bits transmittednumber of erroneous bits receivedBit Error Rate (BER), equal to erroneous bits / total bits.</list>The BER will vary upon the SNR specified, and provides a good indicator of the resilience to noise of a particular modulation scheme. <heading>Convolutional Coding</heading>At this stage the model was split into hierarchical blocks, keeping the modulation and coding parts separate as shown in Figure 3.3. The contents of the newly added 'baseband coding' and 'baseband decoding' blocks are shown in Figure 3.4 and Figure 3.5. <heading>Bernoulli Binary Generator</heading>The samples per frame parameter was increased in order to set the packet size. A value of 120 samples per frame was used to create a packet size of 128 once the zeros had been added for state-pinning (see section 2.4). <figure/><figure/><figure/><heading>Constant & Matrix Concatenation</heading>The constant block provides eight zeros which are padded onto the end of each packet of bits. These zeros ultimately increase the accuracy of the Viterbi decoding because the start of the traceback is known to be zero (see packets and state pinning, section 2.4). <heading>Convolutional Encoder</heading>The Simulink block performs the convolutional encoding in one step. It was only necessary to fill in the 'trellis structure' parameter, which defines the type of convolutional code. <heading>Puncture</heading>Recalling that a 4/3 puncturing method should be used in order to obtain an overall 2/3 puncturing rate, a parameter of [1 1 1 0]' was supplied to the puncture block. The result is that every fourth bit is completely discarded. <heading>Unipolar to Bipolar</heading>The previous system block is the integer to bit converter at the end of the demodulation process. As one would expect, this outputs 1s and zeros (unipolar form). However, the Simulink Viterbi Decoder interprets zeros as bits that have been corrupted or punctured. The output of the demodulator must first be converted into bipolar form, which uses +1 and -1, for the Viterbi decoder to correctly understand the data. <heading>Insert Zero</heading>After every third bit, a zero was inserted to replace the bits stripped out during puncturing. The [1 1 1 0]' parameter was once again used. <heading>Viterbi Decoder</heading>The Viterbi decoder must have an identical trellis structure to the encoder. Also, the traceback length was set equal to the packet size, which is 128 including the state-pinning zeros. <heading>Selector</heading>Removes the 8 zeros previously added to the end of each packet for state-pinning. <heading>CRC</heading>Figure 3.6 shows how the Baseband Coding block was altered to include a CRC in each packet. Simulink only requires to know that a 16-bit CRC should be carried out, it is automatically concatenated with the input stream. Puncturing was removed at this stage, as there was no requirement to demonstrate the effect of puncturing and a CRC simultaneously. Figure 3.7 shows the corresponding CRC detector added into the Baseband Decoding block. It has two outputs. The decoding out is unaffected by the CRC. The pass_fail was used along with another 'Error Rate Calculation' block, which can be seen in Figure 3.3, to calculate the PER. <figure/><figure/><heading>Multi-carrier modulation</heading>The OFDM process is complicated and to implement it all would take more time than would be available. A complete 802.11a system model is available in a Simulink demo, and this was used as the working OFDM model for this project. It is shown in Figure 3.8. There are some key points to note about the system. The system has feedback. Both ends of a wireless communication can act as a receiver and transmitter. This enables two features, retransmission requests, and automatic selection of a modulation scheme. The exact operation of these features is governed by the data link layer, not the physical layer that is being studied in this project. To enable this automatic selection of modulation schemes, there are 8 different modulator types inside the 'Modulator Bank' block: <list>BPSK 1/2BPSK 3/4QPSK 1/2QPSK 3/416-QAM 1/216-QAM 3/464-QAM 2/364-QAM 3/4</list>The fraction represents the overall coding rate of the scheme. The convolutional code is identical in all 8 schemes, only the 2/3 and 3/4 rates include some puncturing. After data has been modulated it is made into frames for OFDM to be carried out on. Each frame is also given 'pilot' and 'training' signals, which are known at the receiver. Any difference between the expected and the received can be used at the receiver to correct for phase shifts caused in the channel, known as equalization. The process of modulating data onto several different carriers is done by an IFFT, which can only be performed on data in blocks of size 2 n. Therefore each frame of 53x24 bits is padded to 64x24 with zeros. These zeros are later discarded at the receiver, after the corresponding FFT has been carried out. A cyclic prefix is added to each frame. This provides what is known as a guard interval between symbols so that delayed and echoed symbols do not interfere with their successors. The PER produced in the model is for analysis purposes only. In practice, the system would not have direct access to the received message and the original uncoded message, so the PER could not be obtained. The method shown earlier of calculating a PER using a CRC is also not possible, since there is no CRC included in the physical layer of 802.11a. Instead, the system uses an alternative method of judging transmission quality. The demodulated message at the receiver is remodulated. Then the two are compared. The difference in the magnitude and phase of the symbols provides a fairly accurate figure of the noise that was injected by the channel. An estimate of the SNR is calculated, and this is used to select the modulation scheme to be used. There are three types of channel noise which can be modelled using the 'Multipath Channel' block, each includes AWGN: no fading - the SNR will remain constant during simulation flat fading - the amplitude and phase shift of the signal will vary over time. dispersive fading - the amplitude and phase of the signal will change and a varying amount of echo is also applied to the signal. The latter can be used to realistically analyse the 802.11a systems immunity to noise caused by multipath reception. <figure/><heading>Performance Analysis</heading><heading>Single-carrier modulation</heading>This section provides a detailed comparison of the coding and modulation schemes used throughout the project. For the majority of comparisons, the BER curves will be used. A BER curve is a plot of Bit Error Rate vs. Signal to Noise Ratio, and shows how effective a modulation scheme is at providing immunity to noise. Before comparing real results, it is useful to note what the curves mean. Figure 4.1 shows four possible different shapes of BER curve. A system that performs like (a) would be preferable to the system that produced curve (b), since the BER drops much faster, at a lower SNR. For any given SNR, system (a) will perform the same or better than (b). System (c) could be better or worse than (a) depending on the SNR at which the system is likely to operate. It can be seen that at a low SNR (high noise environment), (c) would give fewer bit errors, but at a high SNR (a) would be best. Curve (d) tails off slowly and does not reach zero. This type should be avoided in most cases, since it will produce bit errors even at very low SNR. Armed with this knowledge, the results in Figure 4.2-4.6 can be correctly interpreted. <figure/>Figure 4.2 compares the different modulation schemes when no coding is used. These curves were produced with the model in Figure 3.1. It can be clearly seen that as the complexity of the modulation scheme increases (along with the number of bits per symbol) the curves roll off more slowly, marking a deterioration in performance. This can be explained by remembering that having a large number of bits per symbol increases the data rate, creating a compromise where a modulation scheme with a slower baud rate will have better noise immunity. It may be useful to note at this point that 16-QAM outperforms 16-PSK at any SNR, as expected. Figure 4.3 compares two different convolutional codes. As the employed coding becomes more complex, the BER curve has a steeper roll off, but starts higher, creating overlapping curves. A situation arises similar to curves (a) and (c) in Figure 4.1, meaning that different convolutional codes could be more effective depending on the SNR at which they are expected to operate. It will suffice to state that packetisation and state pinning offer very little gain in terms of BER, hence there is no comparison between the schemes with and without state pinning. Its benefits lie in the complexity of the decoding and improving the time delay associated with the Viterbi traceback algorithm, as explained in section 2.4. Figure 4.4 compares 4 modulation schemes in their NASA coded and uncoded forms. This graph can be used to explain the choices of modulation and coding that are found in the 802.11a standard: <list>BPSK, NASA codingQPSK, NASA coding16-QAM, NASA coding</list>Between the three schemes it is possible to have transmission at any SNR with a BER of below 0.1, without changing the convolution code or resorting to an uncoded scheme. This result means that the convolutional code is fixed, and coding and decoding can be performed by dedicated hardware. This is particularly important for increasing speed in the Viterbi decoder, which is one of the main causes of delay in a digital communications system. The 802.11a standard also includes 64-QAM to further increase the data rate. This comparison does not include 64-QAM only because the scheme is not compatible with the packet size used in the test. Even with convolutional coding, 16-PSK is outperformed by 16-QAM even though the two would give the same data rate. For this reason 16-PSK is not included in the 802.11a standard, and would be an unlikely choice for any future system, unless transmission power was not a constraint. This figure also gives an opportunity to compare several modulation schemes at the same BER, say, 0.02 (2 in 100 bits received incorrectly). It may be worth noting that if some outer coding been added to each packet before convolutional coding was used, then undamaged data could easily be restored from a received signal with BER of 0.02 (See section 6: Recommendations). BPSK, both in its raw form or with NASA coding, performs favourably. Without convolutional coding, the signal only just incurs a BER rate as high as 0.02 at a SNR of 0dB (i.e. equal signal and noise amplitude). The coded scheme can provide BER 0.02 at a SNR of around 3dB. For all other schemes, it can clearly be seen that at 0.02 BER the coded schemes outperform the uncoded schemes, with QPSK seeing the best results. The former of these 2 points raises the interest question of why the coded schemes should ever cause more bit errors than their uncoded counterparts. This would be the case for any modulation scheme if it is used it in a situation with very high noise. The answer relates to the Viterbi decoding algorithm, which estimates the symbols that are most likely to have been sent, based on a traceback route (see section 2.3.2). If the coded bit stream arriving at the Viterbi decoder has more errors than the decoder can account for, then the decoder will form an incorrect traceback path, and bit errors will propagate all the way along the path. Such errors will not be rectified until a new packet is received and the decoder is reset. In this way, it is easy to imagine an entire packet being corrupted by propagating errors. Unfortunately, this is inherent in convolutional coding, because the process transforms symbols based on their predecessors and successors. A look back at Figure 4.3 shows that more complex codes are more susceptible to this problem. Figure 4.5 moves the analysis further forward, comparing NASA coding with state pinning and NASA coding with puncturing to the original uncoded system. It can be seen that puncturing simply shifts the curve to the right, having a roll off which occurs at a higher SNR than a scheme without puncturing. One can imagine that if puncturing is taken beyond a certain level, the curve would become sufficiently shifted that the coding scheme performs worse than the uncoded scheme. In practice this would not happen, as the puncturing rate is rarely fixed. As is the case in 802.11a, the puncturing rate will be altered in order to provide optimal data rates in between the jumps provided by different modulation schemes. Figure 4.6 shows a slightly different comparison using packet error rates. It can be seen that the curves do not always roll off, but can have slight twists. These are the result of the random appearance of bit errors, meaning that PER will not always be proportional to BER. In many systems the PER is used to determine signal quality, because the PER can be determined at the receiver by way of a CRC (see section 2.5) without having access to the original signal. Figure 4.7 plainly demonstrates that the PER and BER are different. The scatter diagram shows the model running in 16-QAM mode, with NASA coding and a CRC. The data points are just straying outside their correct detection zones, creating a BER of 0.004606. This is enough to give a PER of 0.1091, since only one incorrect bit is needed for a packet to become corrupt. If every corrupt packet were discarded, this could be extremely wasteful, and so the PER is often passed to higher layers in the system, along with the data. The application that runs on top of this communications layer can then decide for itself whether to request data retransmission. <figure/><figure/><figure/><figure/><figure/><figure/><heading>Multi-carrier modulation</heading>The first test was to vary the SNR in the Multipath Channel block, and observe how the modulation scheme and data rate change according to the models automatic adaptive control. The results, shown in Table 1, do not exactly match the parameters set up in the model, but are relatively close. Any differences can be put down to the fact that the receiver only estimates the SNR based on the data it receives, and it is this estimation that is used to select a modulation scheme. <table/>Another point to note in Table 1 is the way that each modulation scheme can work at 2 data rates. This is because the model can automatically adjust its puncturing rate to achieve maximum data rate. When designing a communications system similar to 802.11a, which can automatically switch between modulation schemes depending on the level of noise in the channel, a graph similar to that shown back in Figure 4.6 may have been used to select the SNR thresholds. For example, for a notionally acceptable PER of 10%, the SNR ranges extracted from Figure 4.6 would be as in Table 2. <table/>Table 3 gives the full results obtained from the 802.11a system. It shows the PER and bit rate achieved at different SNRs for each type of channel noise. Figure 4.8 Shows these results in a graph. It can be seen that when fading is occuring in the channel, the PER will never reach zero. It must have been decided in the 802.11a standard that it is more beneficial to increase the data rate whenever possible, rather than to aim for 100% data accuracy. Figure 4.9 shows the model operating with no fading in the channel. From the scatter diagram it can be seen that the system is using 64-QAM. The bit rate has gradually stepped upward to reach its maximum of 48Mb/s, roughly proportional the SNR. Figure 4.10 shows the model running with flat fading. The system is still using 64-QAM, but the scatter graph for the unequalised signal shows some interesting features. The square shape of received symbols has been twisted by approximately 45 degrees due to phase shifting in the channel. Also, the points on the scatter have been compressed much closer together because of amplitude fading in the channel. However, the equalization process has eliminated these errors, and the high data rate of 48Mb/s has been maintained. Figure 4.11 show the model operating with dispersive fading. Echoing in the channel has caused these ring effects which are visible in the unequalised signal scatter diagram. It can also been seen how the poor SNR has caused the modulation scheme to change to QPSK and the bit rate has stepped down accordingly. <table/><figure/><figure/><figure/><figure/><heading>Conclusions</heading>It has been seen that the encoding and modulation schemes chosen can have serious implications on the performance of a communications system. There are many more options than the various ones explored in this project, but these ones are currently used in wireless netorking (IEEE802.11), and digital radio/television broadcasting, which makes them obvious choices to describe. Each scheme has its merits and disadvantages, it is not so clear cut that the scheme that allows the fastest data rate can be regarded as the best. If reliable transmission is required at a low SNR (e.g. less than 5dB), there would be little choice available and, out of the modulation schemes used here, BPSK would be used. This will give the slowest data rate, but it would be unrealistic to expect a fast data rate with such poor signal quality. The users of an 802.11a LAN system will often experience far lower data rates than promised by the equipment manufacturers, due to these occasions when the system must switch to its last resort BPSK modulation in order to guarantee error free transmission. Newer wireless standards 802.11b and 802.11g have provision for even worse noise conditions, by employing DBPSK (Differential BPSK) and DQPSK, which offers a reduction in complexity with only a slight loss in performance. DQPSK is also used in DAB (Digital Audio Broadcasting). Further work could perhaps examine these differential modulation techniques in more detail. At the opposite end of the spectrum, some communications require high data rates but can tolerate a lot of errors. The DVB standard defines the modulation and coding schemes that can be used, and offers a lot of flexibility. Some commercial digital television channels will use 64-QAM and puncturing to reduce the amount of bandwidth required by their signal (reducing the licensing costs). Unfortunately, the MPEG2 compression used in DVB does not have the ability to degrade gracefully and so in bad weather these channels can be virtually unwatchable. Another factor which affects the choice of modulation is whether the system will have the ability to request retransmissions. In the performance analysis section, Table 3 and Figure 4.8 showed that the 802.11a standard favours higher data rates over 100% accurate data reception. This is tolerable due to its ability to request retransmission of failed data, meaning that no information can be lost. This approach can only be used where delivery time does not need to be guaranteed. In a time critical system, delivery time is essential and it would be unacceptable to need to retransmit data. These situations warrant the use of more complex codes and modulation schemes to try and salvage as much data from a signal as possible. In the past, when decided which coding and modulation schemes to use for a project, the complexity would have been an important factor. More complex algorithms are more expensive to implement, and slower to execute. Slow execution may not be such an issue in broadcast, where a delay would not be noticed, but any delay in mobile telephone conversation would be a problem. Fortunately, the decreasing cost of hardware decoders and the abundance of available software decoders has resulted in a wave of solutions that can implement complex schemes at low cost. Digital TV set-top boxes and mobile telephones are two obvious examples. Power considerations are also now being overcome, with mobile communications companies working hard to reduce the power consumption for these modulation and coding schemes. The fact that OFDM is a likely candidate for 4G mobile communications demonstrates this well. The debate over which modulation schemes are best is ongoing. <heading>Recommendations</heading>This project offered a look at 5 of the most common modulation techniques, and only 2 convolutional codes. There are many others of equal merit, and further work could include these. Two examples raised in the Conclusion are DBPSK and DQPSK. Wireless LAN only uses convolutional coding, but other wireless communications models include other types of error protection (outer coding) to protect against different types of errors, e.g. sudden bursts of noise. There are also methods of interleaving, whereby bits from different packets are shuffled before modulation. Outer coding and interleaving can, in extreme circumstances, help to restore a packet of data from an extremely violent burst of noise. Further work could perhaps look at the DVB model, which includes both outer coding and interleaving. 802.11 is well known to not perform at the data rates that should be possible. Aside the reasons already discussed, the other layers of the 802.11 standard play a big part. A true 802.11 model would need to take into account the interaction between devices using the correct protocol. One final factor that would be taken into account while defining a system, is user perception of corrupt data. How many errors are acceptable to the human ear/eye. As previously stated, data compressed by an MPEG2 algorithm will suffer badly, as errors will be propagated for a period of time. However, the compression could be increased if this was likely to happen frequently. Uncompressed data may have the ability to degrade gracefully, giving the impression that it has been received correctly. However, human perception is difficult to model. 