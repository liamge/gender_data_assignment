<heading>Part 1</heading><heading>Aims</heading>The aims of the coursework were to develop a robotic arm composed of two thin cuboids. The lower cuboid should pan and tilt, and the second cuboid be attached and able to tilt with respect to the first. The arm should be able to be viewed through three orthographic screens, as well as from a perspective view. The user should be able to pan or tilt the camera in the perspective view, as well as changing the focal length and the distance it is from the origin. The camera should always be upright and pointing towards the origin. The overall aims of the coursework, however, are to gain a deeper understanding of using matrices for linear transforms, as I find I only really understand the subtleties of a concept if I have had direct experience implementing it myself. <heading>Method</heading>Although this coursework followed on from "I, Robot 1", I found I had to almost start the code again from scratch. "I Robot 1" stored the 8 points of a cuboid as a list of lists of co-ordinates. It stored a robot, therefore, as a list of cuboids. Manipulations were achieved by having a function for, for example, rotating a point, and then robots were rotated by many calls of this function. This work is more intelligent. It stores cuboids as a 3*8 homogenous matrix, which stores the x, y and z co-ordinate of each of the 8 points of the cuboid. Robots are stored as a list of cuboids. Instead of iteratively calling a function in a loop to rotate a cuboid, we can now do it by multiplying its matrix with a rotational transformation matrix. This has meant that I don't need to keep track of all of the translations and manipulations I have performed on each cuboid in order that I can undo them each time. The perspective view is merely the result of a perspective transform matrix multiplied by the vertices. The camera position is chosen by rotating and translating the robot before the transform. In other words, as the perspective view is merely a relation between the CCS and the OCS, I have inversely altered the OCS rather than directly altering the CCS for the benefit of this view. This made the code far simpler than trying to move the CCS. I also have added code for 'furthest point' hidden line control, which I didn't get time for in the first assignment. <heading>Testing</heading>I have put the code through a great deal of testing before being satisfied that it works entirely. I did this once I had made an arm of two cuboids. As I knew the size and position of both cuboids, I was able to predict on paper their new location after, for example, panning and tilting the base arm by 20 and 35 degrees respectively, and tilting the upper arm by 5 degrees. I was able to use the modmatrixpr() function to print the transformed locations of the cuboids to the terminal as the code ran and was therefore able to check the cuboids' actual positions with where I had predicted them to be. After I had tested the code with four or five tests of this sort I was confident the code was correct. I used a similar technique to test the perspective transforms. As I worked the manipulations out by hand, his method also gave me the advantage of being able to ensure that my ability to multiply matrices was still there. Indeed, when the first perspective transform test gave a different result to the one I calculated, I found that the error was in my calculation rather than in the code. Correcting this error made the two calculations agree. <heading>Evaluation</heading>As I tested the code comprehensively, I am sure that the matrix mathematics is correct. The program fulfils all of its aims in that the user can control the robot in the specified way, and view it both orthographically and with perspective from a given camera position. In terms of the higher aims, my appreciation of how transformation matrices can manipulate a vector is far deeper. This is most noticeable in that I understand far better the need for applying the transformations in the correct order. Before, I had the opinion that matrices were needed for linear algebra in order to be mathematically elegant, but implementers would usually just use arrays and functions for each element. I now see how powerful and essential matrices are in vision and graphics related software. I think that in my own time before the exam I might add code so that the user can specify a distorted cuboid, and the program will calculate how the original 100*100*100 cube has been rotated, scaled, translated, sheered, and put into perspective to give this result. I think this will leave me with an understanding of determining transforms as deep as my understanding of applying transforms now is. <heading>Code Listing</heading> FORMULA  <heading>Part 2</heading>When modelling a robotic arm that is moving in the real world, it is important to ensure that the model is accurate to the robot. If the computer rotates the arm 60 degrees, due to imperfections in the real world, such as friction and air resistance, the actual arm will rotate very slightly more or less than 60 degrees. As this difference is likely to be minute, it could be possible to ignore it. The problem is that as the arm is moved more and more times, this error will grow and compound until the model of the robot bears little resemblance to the robot itself. This result is as catastrophic as a human with a blindfold operating the robot. We need some form of feedback, therefore. This can be done with sensors, but as the robot gets more and more complex, we would need more and more unobtrusive calibrated sensors. Another way is with vision. <picture/>The computer models a robot as a series of vertices. If a small bright blue dot were to be painted on the point on the otherwise non-blue robot in the real world that correspond with each vertex, then these could be picked up by cameras and compared to the positions of the vertices in the computer model. For example, on the right is a robotic arm, and the second image is a simulated extraction of the cuboids from the 'blue dots' in the image. The computer can then take these 2D points and multiply them by the inverse of the perspective matrix. It can then rotate the resulting 3D model so that it is in line with the CCS. The model can then be translated along the z-axis the distance the camera is away. If the real robot is accurate to the computer model, the resulting points should match those of the model. If not, then we have some work to do to find out how to move the robot to put it back where the model thinks it is. We have a known set of points from the computer model, M. We also now have a set of calculated points of the real robot, R. Assuming that the arms of the robot have not changed shape, there must be a translation, T -1, to get us from R to M.  FORMULA  If we could calculate T -1, and then convert it so that we knew what commands to give the robot, we can move the robot to meet the computer model. As this is a robot, this problem of inverse kinematics is simplified. As presumably the base of the robot is fixed, and the camera is fixed, it has only three degrees of freedom. That is for the based arm to pan (pB) and to tilt (tB), and for the upper arm to tilt (uT). Let's say for simplicity that the bottom of the base arm is at the origin of the OCS. Let's also say that in its starting position the connected end of the upper arm is at position (ux, uy, uz) in the OCS. <figure/>y This is represented in the diagram to the left. The axes are x and y, and z comes directly out of the page. Therefore, pB is a rotation around y, and tB is a rotation around z.  FORMULA  tU is more complicated. tU involves translating (-ux, -uy, -uz), rotating tU, and then translating back (ux, uy, uz). This gives:  FORMULA  The transformation to get from the initial position to an arbitrary position is therefore:  FORMULA  We can take the inverses of each of these to form tU -1, tB -1 and pB -1. The transformation to get from an arbitrary position to the initial position is therefore:  FORMULA  Solving the above would give us one transformation matrix with three unknowns in. It is possible to write a simultaneous equation for each of these unknowns. These can then be solved to find the values of tU, tB and pB. The computer can then move the robot - tU, then - tB, then - pB, which will put the robot into the position to match the computer. If there is any uncertainty about whether this new position is accurate, the process can be repeated. To make this process more accurate, images from two or more cameras could be put together to develop the original 3D model of the robot from the images. The computer would take the output of all the cameras and identify each blue dot on each image that it's visible in, multiply each by the inverse of the perspective matrix for the focal lengths, manipulate each to compensate for the various camera co-ordinate systems, and then average all of the resulting readings for each point. Another method for the arm to know where it is could theoretically be using a camera at the end of the arm. The images from the camera could be then recognised so that the computer would know which direction the arm was facing. This recognition could be by identifying known objects in the room. An example is putting small bright red balls around the room in a pattern, and the location of the red dots in the camera's image would tell the computer the angle of the camera. Another method would be to have a database of images of the room stored in the computer and to match the image given by the camera to the 3 closest images in the database, and perform a weighted average to get the true position. This would mean that the view from the robot could not change significantly. 