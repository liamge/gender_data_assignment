<heading>QUESTION 1</heading><heading>Introduction and methodology</heading>We want to test the hypothesis that the regression coefficients for the first half of the sample (20 observations) are significantly different from that for the second half (20 observations). For this purpose we will conduct a Chow test, which is based on an F-test. Initially, we have the regression model:  FORMULA  We introduce a dummy variable (D t), which we assume it affects both the intercept and the slope. The resulting regression model is now given by:  FORMULA  The dummy variable takes the value 1 for the first half of the sample (20 observations) and the value 0 for the second half of the sample (20 observations). So, the regression functions for the dependent variable in the two half of the sample as resulting from (2) are given by:  FORMULA   FORMULA  The assumption that we have made here is that the regressions for the two half of the sample are completely different, since we have allowed the intercept and slope to differ. We now want to test whether the regression coefficients for the first half of the sample (20 observations) are significantly different from that for the second half (20 observations). In other words, we test the equivalence of the two regressions and therefore we specify the null hypothesis (H 0: δ=0, γ=0), which states that the two regressions are equivalent and the alternative hypothesis (H 1: δ≠0 or γ≠0), which states that the two regressions are not equivalent. To test the H 0 against the H 1, we use a Chow test. The restricted model is represented by equation (1) and it assumes no difference in the intercept and slope coefficients across the two half of the sample. The unrestricted model is represented by equation (2) and it allows the intercept and slope coefficients to differ for the two half of the sample. By running two regressions, one for each model we find the SSE for each one, which we then input in Table 1 together with the values for J, T, K and alpha (number of restrictions, number of observations, number of coefficients and significance level respectively). By replacing these values in Table 1 we get the computed values required to reach a decision. <heading>Results and conclusions</heading>Since F=5.638488309>F c =3.259446306, we reject the null hypothesis and therefore we reach the conclusion that the two regressions are not equivalent. Thus, there is difference in the intercept and slope coefficients of the two regressions which means that we cannot pool the data into one sample and describe them with one common regression model that has the form (Table 1):  FORMULA   FORMULA  Instead of that, the equation describing the regression for the first half of the sample is given by (Table 2):  FORMULA   FORMULA  This regression model implies that if x increases by 1 unit, y will be increased by about 0.41 units. The equation describing the regression for the second half of the sample is given by (Table 2):  FORMULA  This regression model implies that if x increases by 1 unit, y will be increased by about 4.8 units. The same decision can be reached if we use the p-value. Thus, we reject the H 0 because p=0.007408845<0.05 (Table 3). <table/><table/><table/><heading>QUESTION 2</heading><heading>Introduction and methodology</heading>We want to examine whether heteroskedasticity is present. One way for doing that is by plotting the residuals, however, a formal way of testing for heteroskedasticity is the Goldfeld-Quandt test, which is the method applied in the current problem. We specify the null hypothesis (H 0: σ12=σ22), which states that the two subsamples have equal variances and therefore heteroskedasticity does not exist and the alternative hypothesis (H 1: σ12≠σ22), which states that the two subsamples do not have equal variances and therefore heteroskedasticity exists. The data were split in half and ordered based on variance. Then, two regressions were run, using the first half of the data (20 observations) in the first one and the second half of the data (20 observations) in the second regression. The two regressions provided the estimated variances, which were input in Table 2 together with the values for T 1, T 2, K, alpha (number of observations in the first subsample, number of observations in the second subsample, number of coefficients and significance level respectively). By replacing these values in Table 2 we get the computed values required to reach a decision. The GQ value was then compared with F c, the critical value from the F-distribution with (T 1-K) and (T 2-K) degrees of freedom. In case that heteroskedasticity is present in the data we assume that we have proportional heteroskedasticity and perform generalized least squares to correct for this. To do this, we need to change or transform our statistical model into one with homoskedastic errors. The initial statistical model y t=β1+β2x t+e t is divided by the square root of x t and we get the transformed model y t*=β1x t1*+β2x t2*+e t*. The new transformed error term e t* is now homoskedastic. Finally, we run a regression on the transformed model. <heading>Results and conclusions</heading>Since GQ=5.54153314>F c=2.217197134 (Table 4), we reject the null hypothesis of equal variances between the two subsamples and therefore we conclude that heteroskedasticity is present because the variances for all observations are not the same. The same decision can be reached if we use the p-value. Thus, we reject the H 0 because p=0.00034173<0.05 (Table 4). The fact that heteroskedasticity is present in the data means that the least squares estimator is still a linear and unbiased estimator but it is no longer the best linear unbiased estimator (B.L.U.E.). Moreover, the standard errors usually computed for the least squares estimator are incorrect. As a result, confidence intervals and hypothesis tests that use these standard errors may be misleading. The equation that describes the regression of the new transformed model is given by (Table 5):  FORMULA  To sum up, by transforming the variables we have converted a heteroskadastic error model into a homoskedastic error model, but the meaning of the coefficients does not change. So, the transformed regression model implies that if x t1* increases by 1 unit, y t* will be decreased by about 9.14 units. Similarly, if x t2* increases by 1 unit, y t* will be increased by about 11.59 units. <table/><table/><heading>QUESTION 3</heading><heading>Introduction and methodology</heading>We want to test both series of data for the presence of a unit root. For this purpose we will conduct a Dickey-Fuller test for each series of data. Initially, we have the y t variable and we create five more, namely Δy t, t, y t-1, Δy t-1 and Δy t-2. We then run three regressions, which they have the same dependent variable y t but different independent variables. For the first regression we use y t-1 as the independent variable, for the second t and y t-1 and for the last y t-1, Δy t-1 and Δy t-2. The three regressions have the form:  FORMULA  The hypothesis that we want to test are specified as follows: The null hypothesis (H 0: γ=0), which states that the series has a unit root, thus it is nonstationary against the alternative (H 1: γ≠0), which states that the series does not have a unit root, thus it is stationary. The tau statistics that result from the three regressions are compared with the critical values for the Dickey-Fuller test in order to reach a decision. We then want to test whether the first difference of the series is stationary. We test this by running a regression using the first difference of the series as the y variable and Δy t as the x variable using the same type of t-statistic. In case of the presence of the unit root we want to test whether these two series are cointegrated, thus we need to test whether the errors are stationary. For this purpose, we conduct a Dickey-Fuller test on the residuals of the regression between the two series. From the regression output we get the residuals and then create two new variables, delta_e and e_ t-1. We then run a new regression using delta_e as y variable and e_ t-1 as x variable, which has the form:  FORMULA  The tau statistic that results from this regression is compared with the critical value for the cointegration test in order to reach a decision. <heading>Results and conclusions</heading>The regressions (3), (4) and (5) for the first series have the form (Table 6):  FORMULA  The tau statistics (4.846487991, 2.934464877, 1.424061824) are all positive (Table 6) and when compared to the critical values for the Dickey-Fuller test (which are negative), we fail to reject the null hypothesis and conclude that the series have a unit root (nonstationary). Moreover, we do not reject the null hypothesis and conclude that the first difference is nonstationary, because the tau statistic (2.141160617) (Table 7) is positive. The regressions (3), (4) and (5) for the second series have the form (Table 8):  FORMULA  The tau statistics (5.519240913, 3.265707032, 1.890947711) are all positive (Table 8) and when compared to the critical values for the Dickey-Fuller test (which are negative), we fail to reject the null hypothesis and conclude that the series have a unit root (nonstationary). Moreover, we do not reject the null hypothesis and conclude that the first difference is nonstationary, because the tau statistic (2.597557656) (Table 9) is positive. Since the tau statistic (-8.657508067) is greater than the critical value for the 1% significance level (-3.90) (Table 10), we reject the null hypothesis that the least squares residuals are nonstationary and conclude that they are stationary and therefore the two series are cointegrated. The regression (6) has the form (Table 10):  FORMULA  <table/><table/><table/><table/><table/>