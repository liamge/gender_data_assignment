<heading>1. Introduction</heading>Moore's law suggests that by 2050 we will have computers as powerful as human brains 1. A few weeks back a discovery was made that could lead to a quantum computer capable of carrying out a staggering 10 30 simultaneous calculations. The Blue Brain project team plans to be doing cellular level whole brain simulations within 20 years. This essay is about the Grand Challenge in Computing 5 - a long term goal for set for research within computing in Britain. It aims to give us a better idea of the relationship between brain and mind by developing a machine with many of the same capabilities as a human. Its primary target for the next 15-20 years is to: <quote>Demonstrate a robot with some of the general intelligence of a young child, able to navigate a typical home and perform a subset of domestic tasks, including some collaborative and communicative tasks. It should know what it is doing and why, be able to cooperate with others and discuss alternate ways of doing things. It should have the linguistic skills to discuss things happening in the world and their implications, including some capabilities to understand the motives and feelings of humans. It could be tested in various practical tasks, including helping a disabled or blind person to cope without human help.2</quote>A research team at the Neuroscience Institute already claim to have developed a robot with some of the intelligence of an 18 month old baby 3. They are just one of many research teams working on problems relating to GC5. They work from many different angles but they can broadly be split into two groups, Top-Down - being those teams looking at simulating and finding out more about existing brains, and Bottom-Up, being those which are working on intelligence from first principles upwards. While the actual challenge documentation considers a third group bringing them together I will be looking solely at the two edge ones in this essay. <heading>2. Top-Down Research</heading>As relatively little is actually know about the precise structure and functioning of the brain, lots of work is going into simulating them, to save the difficultly of working with real brains. When looking at real brains there are many issues at stake: moral issues, time issues, and practicality issues. There are situations in which an fMRi scanner cannot monitor experiments which cannot morally or legally be performed etc. Thus computational modeling has come in to help out with this. <heading>2.1 Case Study: The Darwin Series</heading>The Darwin's are a series of robots developed at the Neuroscience Institute in California by Gerald Edelman and Jeffery Krichmar. They are known as BBDs (Brain Based Devices) and are designed as heuristic bases for the testing of theories of brain functioning 4. Their use lies in that they are modeled on real brain structures, while the activity of every neuron can be monitored while they perform an activity - something impossible in living animals. The data on real brain structures was gathered through functional MRI scans of brains showing points of activity in response to different sights or smells, and neural connections mapped by injecting single nerve cells with dye and then, by microscope, tracing the cell's hundreds of branches that lit up. The important thing about these BBD's is that where traditional ANN's have focused on a small network to perform a single task, these take whole groups of functional clusters and put them together. The interesting thing for researchers is both the behaviour that emerges, and for Gerald Edelman - forefront researcher and theorist in consciousness - it's how they emerge. Darwin VIII has a cortical area of 53,450 simulated neuronal units with 1.7 million synaptic connections and was designed specifically for the problem of how functionally segregated brain regions coordinate to link various features of individual objects while still differentiating between objects. They examined this problem over the course of several experiments which involved such things as playing football, and classical conditioning tests in the style of Pavlov's Dogs. <figure/>The team has shown that different robots learning to recognize the same object may recruit entirely different sets of neurons to do so. In this respect, the robot's mind is a faithful approximation to a living brain. Another example of this was discovered while using the latest model - Darwin X, which they had been studying to discover how brains use landmarks to navigate. They found out they developed "place neurons" in its "hippocampus" which helped it home in on the landmark. This feature had not been planned, or coded in, but simply emerged. It is an occurrence also known to happen in the hippocampal region of rats. <heading>2.4 Case Study: Blue Brain</heading>The Blue Brain project is an international effort to develop a large scale computational model of how the brain - in its entirety works 5. The main core of its work is being done at the EPFL on a large number of Blue Gene super computers. The aim of the work is to be able to do neuroscience experiments on a computer instead of performing slow and costly wetware experiments. They hope to soon be able to invite researchers to build their own models of different brain regions in different species and at different levels of detail using Blue Brain Software for simulation. These models will be deposited in an Internet Database from which Blue Brain software can extract and connect models together to build brain regions and begin the first whole brain simulations. The project has been split into several smaller projects which need completing before the next can continue. These are: <list>Blue Synapse: A molecular level model of a single synapseBlue Neuron: A molecular level model of a single neuronBlue Column: A cellular level model of the neocortical columns (a structure with approx. 10,000 neurons which build up most of the neocortex)Blue Neocortex: Using simplified blue columns, whole neocortical regions and then eventually the entire neocortex will be modeledBlue Brain: With Blue Neocortex complete, other models of cortical, sub-cortical, sensory and motor organ areas will be built</list><picture/><heading>3. Bottom-Up Research</heading>Bottom-Up research is done on a different set of principles to Top-Down. This time the researchers start with an idea of something that they wish to do and produce a robot to do it based on the principles of systems and mathematics etc. These tend to be small, single purpose devices such as iRobots Roomba, a small, single purpose robot fitted with wall sensors, depth sensors and visual systems - all there with the soul purpose of cleaning people's floor 6. It has a sister robot, called Scooba, which is very similar but this one mops floors instead of vacuuming. <heading>3.1 Swarm Intelligence</heading>Swarm intelligence is an important idea that has begun to take off over the last few decades, and is an area believed to have great potential. The concept is essentially that from a swarm of individually stupid units you can get a whole capable of "intelligent" behaviour 7. It originated from watching social insects in nature such as ants and bees, some of which are capable of very advance behaviour, such as herding insects, growing fungus, capturing slaves and building air conditioning into their nests. The Leaf Cutter Ant for instance chews up leaves and lay the remains down in beds, which they sow with the spores of a fungus which is capable of digesting the leaves - they then eat the fungus 8. This lead to a new field of research, amongst which were contained "Ant Colony Optimization", "Particle Swarm Optimization" and "Swarm Robotics"9. The latest of NASA's centennial competitions may provide an interesting testing ground for swarm robotics. The Telerobotic Construction Challenge offers an initial $250,000 reward form the development of semi-autonomous robots that can build complicated structures with minimal remote guidance from human controllers. With the well known ability of ant colonies to build complex nests, it would be interesting to see how swarm bots could be applied to the challenge. <picture/><heading>3.3 Case Study: Swarm Bots</heading>The swarm bots project is a recently completed European effort to develop an artefact composed of a number of simpler, insect-like, robots(s-bots), built out of relatively cheap components, capable of self-assembling and self-organising to adapt to its environment.10 Each s-bot is a fully autonomous mobile robot capable of performing basic tasks such as autonomous navigation, perception of its surrounding environment, and grasping of objects. An s-bot is also able to communicate with other peer units and physically join either rigidly or flexibly to them, thus forming a swarm-bot. A swarm-bot is capable of performing exploration, navigation and transportation of heavy objects on very rough terrains, when a single s-bot has major problems at achieving the task alone. Swarms were trained to work together through the use of genetic algorithms. This has given them ability to move over holes, in formation and over rough terrain along with hunting down "prey" and collective transportation of large objects. They are currently the most advanced swarm robots in the world. <picture/><heading>4. Conclusion</heading>From these few projects I have mentioned here you can get a view on the state of the art in the field. It is a very limited view as I deliberately left out information on whole swathes of research, such as visual systems which tend to blur the boundaries between top down and bottom-up. The point I was trying to get over was that much of the research is steaming on ahead - in completely different directions. And this I believe highlights a fundamental flaw in the nature of this Grand Challenge. While on one hand it calls for research into brain/mind and how intelligence arises, it on the other hand calls for the development of household robotics as its target. While it is a reasonable assumption to say that research into robotics and artificial intelligence is sure to bolster our knowledge on cognitive neuroscience and visa versa, it is not a good plan to link them together into one combined project. The primary reason for this dubiousness with which I treat this merging of these two areas of research is the loss of scientific freedom caused by it - even if this loss is completely unintentional. One way this could come about would be, for instance, if grants for research in the field were only being granted to ambitious projects if they could relate themselves to a Grand Challenge, what would happen then? As far as I can see, projects looking into areas such as Swarm Robotics would be crushed, as while they have already proved themselves in nature, they are not based on the same model of intelligence as humans, and thus of little actual use to the Grand Challenge in its current form. For an example of this close minded human-centrism already infiltrating the project let us look again at its 15/20 year target. It wishes to develop a robot capable of doing many different tasks, conversing with the owner, understanding why it is doing things etc. Consider an intelligent house based on a central server, an extensive sensor network around the area comprising of audio, visual, infrared etc. It would have many ways of outputting information - speakers, alarms, visual display units, direct control of appliances etc. It would have direct control over swarms of small robots, capable of doing all the cleaning, gardening, washing etc. The owner would be able to ask the house to do anything, from any area of the house; the house would be able to predict that the owner wanted something done and act on it instantly. It would be able to permanently monitor the state of the house, and make adjustments based on this information. It would not however fit within the wording of the 15/20 year target for this grand challenge. Now let us compare this idea to that more directly specified by the target - that of a single robot capable of performing all these things. It would only be able to monitor the state of the room it was in, it would be very expensive as it would have to be capable of doing many different tasks using the same body. It would get in the way, as it would have to be on the large side to just perform most tasks required in a human home - and that's not even considering the size it would have to be to contain a powerful enough computer, enough sensors to get relevant data and all the control mechanisms. Importantly it would take time to maneuver around the house and you would have to be in the same vicinity of it to give it commands. Now I do not mean to knock this idea to heavily. A robot of this type would certainly have advantages but the important point here is that the challenge had imposed an unnecessary restriction on the development of robotics. While this is understandable in the context of the challenge it makes no sense in any other context. To develop a robot in a less than efficient way is a ridiculous use of resources - and realistically would never get the funding in the first place. Then we come too actually considering the realism of being able to develop a robot from top down principles capable of doing all this, within 20 years. Let us consider the BBD's of Krichmar and his team - he has claimed that they nowadays have the intellectual powers of an 18 th month old baby. A human baby of this age should be beginning to develop language and a theory of mind and possession. Their robots on the other hand are merely capable of such primal things as navigation and classical conditioning. Compare this to a Bee - which when captured, can be taken by car to a location 6 miles away and find its way back, this is about the ability of the Darwin's. So from this stage they are expected to be able to learn to use tools, and act out of pure altruism for some human "master" - all within 20 years. I cannot help but feel that this is completely unrealistic even if just for the last reason. To be of any real use to Neuroscience, this altruism would need to be a purely emergent behaviour from an ANN similar in science and architecture to a mammalian brain. While this is almost certainly possible, it is something completely unlike anything developed in any higher life form. In fact, the only examples in biology known to act so altruistically are social insects - leading us back to Swarm Intelligence and away from the top-down approach to robotics. So the chances of us being able to do anything useful with top-down research in robotics in the foreseeable future are very low. This is the second reason why I propose that the Grand Challenge is flawed, and it leads directly from the flaws set down in the first. It is simply unrealistic to expect research to progress in such a way that they are mutually beneficial to both sides of the project in the foreseeable future. As criticism alone is worthless lest it is constructive, I shall not end this argument here. I shall instead propose two new Grand Challenges to take the place of this existing one - one doing computational modeling of brains and the models of related systems for purely neuroscience reasons, the other working towards robotics which would be useful in the home for instance. Using this method there could be set reasonable and attainable goals and a realistic roadmap for the challenges progression could be laid down, and there would be no pretense of them assisting each other. Some people may say that the first challenge I propose, being purely for the purpose of Neuroscience is not really a challenge in computing. But I deny this forthright. When we look at Neuroscience, what is to be seen but the greatest reverse engineering project ever to be undertaken? The brain was natures answer to computers, and thus those who are trying to work out how it works are but Computer Scientists of a different persuasion. The subject of Biology covers everything from the smallest bacterium through to Gaia in her entirety. Maybe it is time that the two sides of computing united towards a common purpose, without crossed-purposes muddling the relationship. 