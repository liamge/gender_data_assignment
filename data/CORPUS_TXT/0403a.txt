<heading>Coding Theory</heading><heading>Introduction</heading>In the 21 st century we expect electronic equipment to work. When we load documents, view pictures, play music or print letters, we expect everything to be correct. We don't want to see random characters in the middle of a 20 page report or hear clicks in our favourite song. Electronic equipment however is prone to errors - frequently things go wrong in the binary code that is transmitted inside a computer, which causes a 1 to be read as a 0 or vice-versa. Computers are not the only problem; sending information across vast distances such as space or a phone line faces the same difficulty. <heading>Motivation</heading>Imagine the following situation where a sender wants to transmit 1 byte (8 bits) of data to a receiver via the 'airwaves': <picture/>Assuming there is a possibility that some part of the message could have been changed, how does the receiver know that '01111010' is what the sender transmitted? This is the motivation; solving these problems is what Coding theory is all about. <heading>A brief history of Coding Theory</heading>It can be argued that the 'father' of Coding Theory is Richard Hamming. Hamming worked at the Bell laboratories on mechanical relay machines. The computers he worked on could detect an error but could not fix it and would abandon the current job. As Hamming only had access to the computer at limited times this was very frustrating. Hamming developed a single error correcting code which enabled the computer to detect and correct one error in a code. Other mathematicians and programmers who shared Hamming's problems developed new and more complicated codes that would allow multiple errors to be corrected. There are now many different types of codes and each one is suited to a particular job. Compact Discs have interleaved codes, which can correct up to 4,000 consecutive errors. A complicated code would preserve data very well, but it would result in lower efficiency as extra information would have to be stored that had no relevance to the original 'message'. However, if the risk of data loss is small then a simple code would do. <heading>Codes, codewords and code alphabets</heading>Definition: Let A = {a 1, a 2, ..., a r} be a finite set, which we call a code alphabet. Then an r-ary code (bin-ary, tern-ary etc) over A is a subset C of the set A* of all words over A. The elements of C are called codewords. The number r is called the radix of the code. Binary is made up of 0's and 1's. So for binary, the code alphabet is {0, 1}. The radix of binary is 2. For most of this essay I will be working with the part of Coding theory that is associated with binary. <heading>Modelling errors</heading>There are many ways of modelling errors within a message; one model is white noise. Most people know white noise as the sound their TV makes at the end of a video. The sound coming out of the speaker is completely random; it sounds chaotic. White noise within computer circuits, telephone lines and radio transmissions is very similar to this. Random changes happen to the information that is being sent. There are two assumptions for the white noise model: The probability of an error occurring at position x is exactly the same as an error occurring at any other position. This probability is p. Errors in different positions are independent of each other. In real life, equipment tends to malfunction at a particular time, thus contradicting the idea that errors are independent of each other, however it is impossible to predict when a piece of equipment will malfunction, therefore the white noise model is a reasonable assumption. The probability of an error occurring at position x is p. The probability of no errors in n positions is: (1-p)n The probability of one error is: np(1-p)n-1 The probability of k errors is:  FORMULA  The probability of i events happening is modelled by the Binomial Distribution:  FORMULA ,  FORMULA . <heading>Detecting errors - The parity check</heading>If the electronic equipment can repeat a transmission, then just detecting an error would be enough. We could then simply repeat the transmission until we get it right. A simple error checking code is a parity check. Here, we count the number of 1's in a codeword, and append a 0 if it is an even number and a 1 if it is odd. For example: <table/>This codeword can then be sent and when it reaches the other end, a parity check can be run. In the case of '1100', we would send '11000'. If an error occurred in transmission and '11010' was received, the parity would not match so we could ask for the data to be sent again. However with this method no two errors (or in fact, any even number of errors) would be detected. <heading>Detecting errors - repetition codes</heading>Another relatively simple code is a repetition code. This involves sending each codeword more than once. Sending n copies of our codeword allows detection of n-1 errors. For a block of binary with n digits, there are 2 n codewords available. For example, a 3-block binary code has 8 codewords: <quote>000, 001, 010, 011, 100, 101, 110, 111</quote>If our code word was '0110' then we would send '01100110'. If the symmetry of our new codeword is destroyed in transmission then we would know that an error had occurred. In the case above, if '01101110' was received, we would know an error occurred. Again, like the Parity check, this error detecting code has no way of detecting even numbers of errors if two errors occur that preserve symmetry: <picture/>This method of error checking clearly causes a decrease in efficiency. Digits are sent that contribute nothing to the meaning of the message. Now that we have two different codes, it would be useful to have some way of comparing them. <heading>Redundancy</heading>In the repetition codes discussed above, a certain number of digits convey no extra meaning. We define redundancy as the number of digits used divided by the minimum necessary. So if we were sending 0110, the minimum number of digits necessary would be 4. If we were sending it by a repetition code, then there would be 4 digits not contributing anything to the meaning of the data. For this example, the redundancy is 8 / 2 = 4. For the parity check code, redundancy has a general formula:  FORMULA  As n increases, this redundancy gets smaller, however, its ability to detect errors in a noisy channel decrease. <heading>Information rate</heading>The information rate is the fraction of data over the whole message size. In the repetition code above, 4 bits are useful data, and the rest useless. Therefore the information rate is 4 / 8 = 0.50. In the parity check code, the information rate is (n - 1) / n. <heading>Error correcting codes - Rectangular</heading>Hamming thought that if a code can detect an error, then why can it not correct it? This is the basis of error correcting codes. One such code is called the 'Rectangular' error correcting code. For this code to work we have to assume that the probability of two or more errors is very small. If this is true for length of the message we have chosen, then we can put the codeword into a rectangle and use a simple parity check. For example, if we wanted to send the codeword 010101101, we lay it into the white boxes in the square below, from left to right and top to bottom. <table/>The yellow boxes are then filled by parity digits. For the first row '010' is odd which means the parity digit is 1. This is completed for all rows and columns. <table/>Then we would transmit all of the data, reading left to right and top to bottom: <picture/>The red digit on the receiving end shows that an error occurred. However the receiver does not yet know this. If we place the received codeword into a square and run our own parity checks, we can see that two parity digits are wrong: <table/>The parity check failed in row y and column x, which means the error lies in cell (x, y): <table/>We can simply change this cell to a 0, and we now have 010101101, which is the corrected codeword. We need n 2 bits to send this code, but we only use (n-1)2, so the redundancy of the code is:  FORMULA  If the error occurred in any of the parity digits, the bottom right parity digit would be wrong, so no correction on the code would be needed. This code only allows for one error to be corrected; multiple error correcting codes are discussed later. <heading>Error correcting codes - Triangular</heading>Exactly the same principle can be applied but instead of filling a square, we fill a triangle: <table/>Again, this would only work if the chance of two or more errors was very small. If an error occurred and the parity check failed in row y and column x, the error would lie in cell (x, y). The redundancy of this code is:  FORMULA  <heading>Perfect Hamming Codes</heading><heading>The Syndrome</heading>A perfect Hamming code uses a syndrome. In order to explain what a perfect Hamming code is we first need to explore the syndrome. A parity check either returns a 0 or a 1. If we run different parity checks multiple times we can end up with a string of 0's and 1's. This is called a syndrome. We expect a syndrome to be able to tell us one of n + 1 things: The location of any error in one of the n bits of the message The case that no error has occurred If we define the number of parity checks to be m, then we have m bits in the syndrome. Under binary, this means it can take on one of 2 m values. Hence we want:  FORMULA  This inequality is used to decide the minimum number of check digits necessary for a given code. <heading>Hamming Codes</heading>There is an elegant and ultimately beautiful way of creating a code such that the above inequality holds, this is said to be a (n, m) Hamming Code. Later on we will find some of these codes are perfect. In the explanation and demonstration of a Hamming Code, we will use a codeword of length 4, and a binary code alphabet. Hamming suggested using a sequence of parity checks. Using the inequality discussed above we can see that only 3 check digits are needed for a codeword of length 4:  FORMULA  Consequently, somewhere 'out there' Hamming knew that there was a single-error correcting code with an information rate of 0.57 (4 / 7). Hamming's syndrome must take on a value between 1 and 7 to indicate the position of the error or 0 if no error occurred. In binary: <table/>The first parity check creates the first digit of the syndrome (when we say first, we mean the 2 0 digit - which is actually the first digit from the right). Therefore if an error occurred in the check the final syndrome is restricted to having a 1 in this position - 1, 3, 5 and 7. So the first parity check should be done on these positions in the codeword. Applying this logic to the second parity check gives positions 2, 3, 6 and 7 (all syndromes with a 1 in the middle digit). The third party check should be run on positions 4, 5, 6 and 7. <table/>The parity checks have to be independent; no two parity checks should check each other. The only way to do this is to place them in a position that is only checked by the ith parity check - places 1,2 and 4 only have one 1. This leaves places 3, 5, 6 and 7 for our codeword; I will use 1010: <table/>Now we run the first parity check on positions 1, 3, 5, 7: 0 + 1 + 0 + 0 = 1 which is odd, so the parity digit is 1. This digit is placed into position 1: <table/>Now we run the second parity check on positions 2, 3, 6, 7: 0 + 1 + 1 + 0 = 2 which is even, so the parity digit is 0. This digit is placed into position 2: <table/>Now we run the third parity check on positions 4, 5, 6, 7: 0 + 0 + 1 + 0 = 1 which is odd, so the parity digit is 1. This digit is placed into position 4: <table/>How do we know the syndrome indicates the position of an error? Let H be a 3 x 7 matrix whose columns are the binary numbers from 1 to 7:  FORMULA  Notice the links between the places of 1's and the positions we ran each of the three parity checks on. The bottom row corresponds to the first parity check, the middle row to the second, and the top row to the third. If we multiply a codeword on the left by H (and mod 2 on the final 3 x 1 matrix), it is equivalent to running the three parity checks:  FORMULA  The syndrome 0, 0, 0 indicates no error is present. Let us test the code by artificially creating an error in the codeword: <picture/>Now when we multiply on the left by H we get:  FORMULA  This tells us an error occurred in the 2nd digit of the codeword, so we can correct it. <heading>Multiple-error correcting codes</heading>Possibly one of the strongest multiple-error correcting codes is the Reed-Solomon code. This code was invented in 1960 when digital technology at that time was actually not advanced enough to implement the code. It is possible to correct up to 4000 errors on a CD by making use of an interleaved code. The code however has a weakness; if multiple errors occur in a 'symbol' (like a 'byte' of information) then the Reed-Solomon code sees this as one error. This makes the code suitable for lines where errors occur in bursts but inefficient on lines where single errors occur randomly. <heading>Hamming Distance - linking with Metric Spaces</heading>We can define the distance between two codewords of binary as the number of digits in which they differ. For example, the distance between 000 and 111 is 3, and between 010 and 011 is 1. We define the distance function d(x, y) to be the distance between x and y. d(x, y) is a metric because it satisfies all the axioms: <list>d(x, y) >= 0 for all x, yIf d(x, y) = 0 x=yd(x, y) = d(y, x) for all x, yd(x, z) <= d(x, y) + d(y, z) for all x, y, z</list>With the Hamming distance, it is possible to view codes geometrically. <heading>Geometry of the Hamming Distance</heading><heading>Minimum Distance 2</heading>If we use a (2, 1) repetition code where we send each bit twice, then there are 4 possible codewords that could be received, with the middle two being errors:  FORMULA  Placing these into a square using the Hamming Distance between them as the length of the side we get: <picture/>An error in sending the codewords 00 or 11 would send them along one of the edges to 01 or 10. As we are using a (2, 1) repetition code then we only expect 00 or 11, so we know an error occurred. In order for a code to be single error detecting, its minimum distance must be at least 2. Geometrically, this means the distance between 00 and 11. <heading>Minimum Distance 3</heading>If we use a (3, 1) repetition code, there are 8 possible codewords that could be received:  FORMULA  Placing these into a cube, using the Hamming Distance as the length of the sides we get: <picture/>If a single error were to occur, it would send 000 or 111 to a non-codeword symbol; however this non-codeword would still be closer to its original form. This means we could correct the error. In order for a code to be single error correcting its minimum distance must be at least 3. This code could detect two errors. This continues up the minimum distance scale. A minimum distance of n means we can detect n / 2 errors and correct (n - 1) / 2 errors. <heading>The elegant link between Coding Theory, mathematics and oranges</heading>Pure mathematics plays a big part in solving the problems coding theory faces; even in the simplest codes, mathematics is used. Linear Algebra and Metric Spaces are also involved in Coding theory. Codes are the kernel of a linear transformation and it is possible to define a metric between two binary codes. Perfect codes do exist. These perfect codes have a special geometrical property: Given a set of points S in n-dimensional space then disjoint n-dimensional spheres can be placed around about each element of the code such that they contain all of the points of S, each point in exactly ONE sphere. This is known as the 'Sphere Packing Problem' (SPP). The 3-Dimensional SPP is demonstrated in the following formulation: "How can oranges be packed into a box so that as many as possible can be taken?" In other words: 'minimising the unused space'. The (7, 4) Hamming Code, discussed earlier, has these properties of packing spheres so is perfect. <heading>Summary for Coding theory</heading>Within these pages I have explored and explained just part of the complex area of maths known as coding theory. Coding theory will always play a part in our lives, especially in this computer era. Interestingly, coding theory exists outside of computers; nature also chose the 'parity check' for DNA and proteins. Satellites sent into space will begin to use more complex codes so that very weak signals from the vast distances of space can be received and understood and the advances in quantum computing will call for stronger codes to be invented and used. <heading>Cryptography</heading><heading>Introduction</heading>Cryptography is an area of maths that deals with information security. In the modern world this is closely linked with computers, however cryptography existed many years before the traditional computer. Cryptography involves taking a piece of data and encrypting it, using a key, to make the data impossible to read unless the user can decrypt the data. This can be compared to putting a piece of information into a box and locking it. Only somebody with the same key could unlock the box and read the information. <heading>Motivation</heading>Secrets between people have existed probably since the start of mankind. Sometimes secrets have to be passed on to a particular person; how do we do this without letting anyone else hear our secret? Naturally this is quite hard; people can overhear a conversation or intercept a letter. Is it possible to come up with a system so that even if somebody heard or saw our secret, they would not be able to understand it? People quickly found that it was possible, either by creating a new language or coming up with a 'code' that nobody could understand unless you were taught it. <heading>The early days of Cryptography</heading>In the beginning, cryptography was concerned with language. Most codes were language based, this means that all that was really needed was a pen and paper. There were two main systems in use; transposition ciphers and substitution ciphers. Transposition ciphers involved taking a message and rearranging the letters in the message and substitution ciphers entailed replacing letters by other letters, or indeed groups of letters. <heading>Caesar Cipher</heading>One of the earliest ciphers was the Caesar cipher. This involved shifting each letter in the plain text (the original message) along the alphabet a certain number of times. Caesar used a shift of three. For example, A would go to D, B to E, C to F... Z to C. This makes the Caesar cipher a substitution cipher. Multiple encryptions would add no more security to the data, as a shift of 1 applied twice is exactly the same as applying a shift of 2 only once. Unfortunately, we have no way of knowing how successful the Caesar cipher was at keeping secrets safe. In Caesars favour, most of the people at that time could not read, let alone create some way to decipher his messages. Frequency analysis was discovered in the 9 th Century AD. It involves counting each letter in the message and tallying up the totals. For English, 'e' is the most common letter. Shift ciphers, (the general form of a Caesar cipher) can be broken using frequency analysis, the most common letter in the cipher text would be the equivalent of an 'e'. It would be possible to then count the distance between this letter and 'e', giving the shift value. Brute force can also be used to break shift ciphers. If 'jgnnq' was the cipher text, we could draw up a table cracking the code: <table/>Immediately to the human eye the plaintext 'hello' stands out. <heading>The Ancient Greek scytale</heading>This is a scytale, probably one of the first transposition ciphers invented. A tape would be wrapped around a stick and the message written down one side. Random letters would then be filled around the other sides so when the tape was unravelled the original message is completely hidden. <picture/>Of course if you had no idea how the message was encrypted this would be hard to break. If you did know a scytale was used however, you could read the message using different spacing between letters...or just grab the nearest stick. <heading>Vigenère Cipher</heading>The Vigenère cipher is similar to the Caesar cipher but uses a different shift according to the place in the 'key'. It was invented in 1553 by Giovan Batista Belaso, but was formally given its (incorrect) name 'Vigenère Cipher' in the 19 th Century. A Vigenère square is used to encode and decode the messages: <picture/>If we wanted to encrypt the text "Cryptography" we would choose any key as long as both parties knew it. We will use "MATHS". The key is repeatedly written out below the plain text. To encode the letter 'C' we would look down the column C and across the row 'M' until they met; giving us 'O'. This is repeated for the whole message: This gives the cipher text 'ORRWLAGKHHTY'. This method of encryption does not make the cipher text unbreakable when using frequency analysis as discussed earlier, just slightly harder. <table/>The opposite method would be used to decrypt the message. The main concept with the Vigenère cipher is, the longer the key, the harder it is to break. Shortly after the Vigenère cipher was invented, the 'one time pad' concept was born. This combined using a Vigenère cipher with a key that was entirely random and as long as the original message. This proved to be mathematically unbreakable. It however faced certain downfalls: It was very hard for the recipient of the message to be given the 'one time pad' (the key) without it falling into the wrong hands. A true random one time pad was near impossible to create - humans can only be 'pseudo random'. Tests were later invented to help determine the length of the key used; this greatly helps any code-breaker when trying to decrypt a message that uses the Vigenère cipher. <heading>The idea behind the perfect cipher</heading>While frequency analysis proved to be a powerful tool with code breaking, the Vigenère cipher and others like it were still used often. There were so many ciphers in use that the code breaker would have to try many techniques before they found the right one. However the perfect cipher would be one where even if the code breaker knew it had been used it was still secure. This idea is known as "Kerchoff's Law". <heading>Secrets and even bigger secrets</heading>I believe if you asked random people in the street the words they associated with "cryptography", somewhere in the top 10 most common words "Enigma" would appear. A significant contribution to our success in the Second World War came from our mathematicians in Bletchley Park, helping break important German secrets. The Germans however had a much more powerful and subtle cipher. It was known as the Geheimschreiber. This translates to "the secret writer". The Enigma machine required an operator to manually type the message into a typewriter keyboard and note down the corresponding letters that were displayed. The Geheimschreiber was much more efficient, taking teleprinter input and sending its output to the receiver's Geheimschreiber machine ready for automatic decryption. The need to break the output of the Geheimschreiber machines output led Bletchley Park to create the worlds first large scale computer, known as the Colossus. The code was broken, and provided the Allies a much needed insight into the mind of the Third Reich. <heading>Cryptography now</heading>There are two main areas of modern cryptography, symmetric key and public (asymmetric) key cryptography. <heading>Symmetric Key</heading>Put simply, symmetric key cryptography describes an encryption system where both 'Alice' and 'Bob' (names frequently used in Cryptography instead of person 'A' and person 'B' to aid understanding) have the same key. This was understood to be the only method of encryption until 1976. Block ciphers take a message and output encrypted text of the same length. This is deemed not to be secure enough for today's standards, as an encrypted text should never be the same twice. One such cipher is DES (Data Encryption Standard) invented by IBM in 1975. It was later disregarded as a single key could be broken in less than 24 hours by using a brute force attack. A stronger form AES (Advanced Encryption Standard) was first published in 1998. This proved so strong the US Government approved its use on classified information. It has been claimed that AES is breakable, but designers of AES looked at the proposed 'break' and were quick to comment on some of the hackers estimates for the time needed. One technique that was previously used on the internet to exchange secret information involved a 'double padlock' technique. Alice would lock her message with her own padlock and key and then post this package to Bob. Bob would lock this package with his (different) padlock and key and post it back to Alice. Alice would then unlock her padlock and send the package back to Bob. This would only leave Bob's padlock on the package, so Bob could unlock it with his key. At no stage could anyone read the information and also Alice and Bob did not have to exchange keys - a risky event. This method was slow and required many transmissions of the data, increasing the chance it would be intercepted. Another type of encryption was necessary. <heading>Public Key Cryptography</heading>Public Key cryptography is also known as asymmetric key cryptography, because of the difference between the keys needed for encryption and decryption. It is hard to imagine a padlock where you need two different keys to lock and unlock it, but the idea is relatively simple. Alice would calculate the key to encrypt the data and using another method, calculate the appropriate related decryption key. Alice can then safely transmit the encryption key knowing it does not matter whose hands this information falls into. Bob would then encrypt his message using the encryption (Public) key and transmit the message back to Alice. Alice can now use her private key to decrypt the message. A popular cryptosystem using Public Key cryptography is RSA. It was published in 1976 although GCHQ in England later announced they had secretly discovered it in 1970. The name RSA comes from its inventors: Ron Rivest, Adi Shamir and Len Adleman. To generate the public and private key, Alice needs to do 5 operations: Choose two large prime numbers p and q such that p ≠ q, randomly and independently of each other Compute n = pq Compute the totient Φ(n) = (p - 1)(q - 1) Choose an integer e such that 1 < e < Φ(n) which is coprime to Φ(n) Compute d such that de ≡ 1 ( mode Φ(n)) When these steps are complete, the public key consists of n and e. The private key consists of n and d. To encrypt a message, Bob would turn his message into a number m. With m < n, to do this he would use a padding scheme that had been agreed upon previously. The encrypted message 'c' is calculated as follows:  FORMULA  Bob knows n and e, as they are included in the Public key. Bob can now transmit 'c' safely to Alice. To decrypt the message, Alice uses the following calculation:  FORMULA  Alice previously calculated d, as it forms part of her private key. Using m she can find M, the original message. <heading>The future of Cryptography</heading>Computer speed is growing exponentially, every year computing speed quadruples at the very least. Is there a ceiling to computing speed? With the predicted upcoming advance into quantum computing most people would argue that if there is a ceiling, it is still a very long way off. This is the main concern to cryptography. Cryptanalysis is the art of breaking codes, either by finding a weakness or using brute force. The brute force approach is best done with a powerful computer - the faster the computer the quicker the code is broken. No matter how powerful a computer is however, some cryptography techniques are unbreakable, for example the one-time pad technique. If a code is not 100% unbreakable, the operations needed to decrypt it will be exponential to the number of operations needed to encrypt it. Not all cryptanalysis methods require exploiting weaknesses - a side-channel attack involves analysing the time it takes to encrypt a message. Knowing this you may be able to deduce which cipher was used. 30 years ago, we did not know Public Key cryptography was possible, because of this the future of cryptography is unclear. I predict a 'key' will always be necessary but that the types of key we use will change. Imagine a system where the key is your fingerprint or an iris scan - hard to implement but equally hard to break. <heading>Summary</heading>In many countries Cryptography is illegal. The government will set an encryption standard so that if needed they could read all messages being sent within their networks. America does not let people export encryption products that use a key of longer than 40-bits - a very weak encryption. However, Governments themselves are hungry for cryptography, often pouring money into its research - they want their secrets to stay secret. Parallel to this, a Government will also have a team investigating cryptanalysis, just like the people at Bletchley Park during the War. The reason for this as Robin Morgan (a poet and writer) says is 'Information is power'. 